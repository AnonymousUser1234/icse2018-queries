mahout-0.8
MAHOUT-1410
https://issues.apache.org/jira/browse/MAHOUT-1410
clusteredPoints do not contain a vector id When clustering non-named vectors there are no vector ids in clusteredPoints so the other values there, cluster id, vector values, distance-squared, pdf, cannot be tied to any known vector.
do not contain a vector id When clustering non-named vectors there are no vector ids in so the other values there, cluster id, vector values, distance-squared, pdf, cannot be tied to any known vector.
******
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationDriver.ClusterClassificationDriver.selectCluster(Path, List<Cluster>, ClusterClassifier, Path, Double, boolean), true
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationDriver.ClusterClassificationDriver.classifyAndWrite(List<Cluster>, Double, boolean, SequenceFile.Writer, VectorWritable, Vector), true
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationDriver.ClusterClassificationDriver.writeAllAboveThreshold(List<Cluster>, Double, SequenceFile.Writer, VectorWritable, Vector), true
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationDriver.ClusterClassificationDriver.write(List<Cluster>, SequenceFile.Writer, WeightedVectorWritable, int), true
core.src.test.java.org.apache.mahout.clustering.classify.ClusterClassificationDriverTest.ClusterClassificationDriverTest.checkClustersWithOutlierRemoval(), false, test_method
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationMapper.ClusterClassificationMapper.map(WritableComparable<?>, VectorWritable, Context), true
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationMapper.ClusterClassificationMapper.write(VectorWritable, Context, int, double), true
#####
mahout-0.8
MAHOUT-1409
https://issues.apache.org/jira/browse/MAHOUT-1409
MatrixVectorView has index check error There is a > in the test for the correct index where there should be a >=
has index check error There is a > in the test for the correct index where there should be a >=
******
math.src.main.java.org.apache.mahout.math.MatrixVectorView.MatrixVectorView.MatrixVectorView(Matrix, int, int, int, int), true
math.src.test.java.org.apache.mahout.math.MatrixVectorViewTest.MatrixVectorViewTest.testIndexRange(), false, new_method
#####
mahout-0.8
MAHOUT-1398
https://issues.apache.org/jira/browse/MAHOUT-1398
FileDataModel should provide a constructor with a delimiterPattern For now we only have ',' and '\t' as delimiters, this is really not enough for users. Of course users can overwritten processLine etc. to archive their goal(e.g. use four spaces as delimiter pattern), but as a well designed framework, Mahout should consider vary demands of most users and make it very easy to use. Also, it will not cost much time to implement, can I push a patch on this ?
should provide a constructor with a For now we only have ',' and '\t' as delimiters, this is really not enough for users. Of course users can overwritten etc. to archive their goal(e.g. use four spaces as delimiter pattern), but as a well designed framework, Mahout should consider vary demands of most users and make it very easy to use. Also, it will not cost much time to implement, can I push a patch on this ?
******
core.src.test.java.org.apache.mahout.cf.taste.impl.model.file.FileDataModelTest.FileDataModelTest.testReadRegexSplittedFile(), false, new_method
core.src.main.java.org.apache.mahout.cf.taste.impl.model.file.FileDataModel.FileDataModel.FileDataModel(File, String), false, new_method
core.src.main.java.org.apache.mahout.cf.taste.impl.model.file.FileDataModel.FileDataModel.getDelimiter(), true
core.src.main.java.org.apache.mahout.cf.taste.impl.model.file.FileDataModel.FileDataModel.FileDataModel(File, boolean, long), true
#####
mahout-0.8
MAHOUT-1396
https://issues.apache.org/jira/browse/MAHOUT-1396
Accidental use of commons-math won't work with next Hadoop 2 release The project uses commons-math3, since about a year ago. However there is a use of old commons-math (2.2) lurking: core/src/main/java/org/apache/mahout/classifier/sgd/TPrior.java: import org.apache.commons.math.special.Gamma; This happens to have worked since commons-math has been pulled in by hadoop-common. But it no longer is in HEAD: http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml?view=markup So this will no longer compile against the latest Hadoop. I believe it will also not actually run again the latest Hadoop, even if one were to use a version compiled versus older Hadoop 2, since the class that uses it is used in the context of Writables - that is, outside the client environment that might happen to have packaged commons-math - and so would fail on the cluster. The change is trivial, to import the commons-math3 class. I've verified that tests pass and a patch is attached. Question is how much of a 'blocker' this should be for the pending release. It would cause it to stop working with the next Hadoop 2 release, so would be useful to get in, IMHO.
=== changed only imported library ===
******
#####
mahout-0.8
MAHOUT-1389
https://issues.apache.org/jira/browse/MAHOUT-1389
Complementary Naive Bayes Classifier not getting called when "-c" option is activated When run trainnb and testnb with "-c" option, Complementary Naive bayes Classifier not getting called.
Complementary Naive Bayes Classifier not getting called when "-c" option is activated When run with "-c" option, Complementary Naive bayes Classifier not getting called.
******
core.src.main.java.org.apache.mahout.classifier.naivebayes.training.TrainNaiveBayesJob.TrainNaiveBayesJob.run(String[]), true
core.src.main.java.org.apache.mahout.classifier.naivebayes.test.TestNaiveBayesDriver.TestNaiveBayesDriver.runMapReduce(Map<String, List<String>>), false, test_method
#####
mahout-0.8
MAHOUT-1379
https://issues.apache.org/jira/browse/MAHOUT-1379
ClusterQualitySummarizer fails with the new T-Digest for clusters with 1 data point ClusterQualitySummarizer (with the new t-digest) fails if a cluster has only a single data point. The issue is the call to OnlineSummarizer.getQuartile() expects > 1 point to be available in the cluster.
fails with the new T-Digest for clusters with 1 data point (with the new t-digest) fails if a cluster has only a single data point. The issue is the call to expects > 1 point to be available in the cluster.
******
examples.src.main.java.org.apache.mahout.clustering.streaming.tools.ClusterQualitySummarizer.ClusterQualitySummarizer.printSummaries(List<OnlineSummarizer>, String, PrintWriter), true
examples.src.main.java.org.apache.mahout.clustering.streaming.tools.ClusterQualitySummarizer.ClusterQualitySummarizer.run(String[]), true
examples.src.main.java.org.apache.mahout.clustering.streaming.tools.ClusterQualitySummarizer.ClusterQualitySummarizer.main(String[]), false, method_signature
#####
mahout-0.8
MAHOUT-1378
https://issues.apache.org/jira/browse/MAHOUT-1378
Running Random Forest with Ignored features fails when loading feature descriptor from JSON file Running Random Forest with Ignored features fails when loading feature descriptor from JSON file. in Dataset.java , fromJSON(String json) function line 400 original: ------ nominalValues[i] = array; should be change to nominalValues[i - ignored.size()] = array; // put array in ignore-feature filtered index
Running Random Forest with Ignored features fails when loading feature descriptor from JSON file
******
src.main.java.org.apache.mahout.classifier.df.data.Dataset.Dataset.toJSON(), true
src.main.java.org.apache.mahout.classifier.df.data.Dataset.Dataset.fromJSON(String), true
src.main.java.org.apache.mahout.classifier.df.data.Dataset.Dataset.Map(Attribute, String[], boolean), false, comments
core.src.test.java.org.apache.mahout.classifier.df.data.DatasetTest.DatasetTest.jsonEncoding(), false, test_method
core.src.test.java.org.apache.mahout.classifier.df.data.DatasetTest.DatasetTest.jsonEncodingIgnoreFeatures(), false, new_method
#####
mahout-0.8
MAHOUT-1371
https://issues.apache.org/jira/browse/MAHOUT-1371
Arff loader can misinterprete nominals with integer, real or string If the nominal values contain a value like integer, real or string it will be misinterpreted as such instead of nominal.
loader can misinterprete nominals with integer, real or string If the nominal values contain a value like integer, real or string it will be misinterpreted as such instead of nominal.
******
integration.src.main.java.org.apache.mahout.utils.vectors.arff.ARFFVectorIterable.ARFFVectorIterable.ARFFVectorIterable(Reader, ARFFModel), true
integration.src.main.java.org.apache.mahout.utils.vectors.arff.ARFFIterator.ARFFIterator.computeNext(), true
integration.src.main.java.org.apache.mahout.utils.vectors.arff.ARFFIterator.ARFFIterator.splitCSV(String), true
integration.src.test.java.org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest.ARFFVectorIterableTest.testSparse(), false, test_method
integration.src.test.java.org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest.ARFFVectorIterableTest.testMultipleNoms(), false, test_method
#####
mahout-0.8
MAHOUT-1368
https://issues.apache.org/jira/browse/MAHOUT-1368
Convert OnlineSummarizer to use the new TDigest The new TDigest provides better accuracy for quartile estimation as well as producing any other quantile you might like. The current quartile estimation of the OnlineSummarizer fails for highly skewed distributions and can't really be extended to provide other quantiles. The TDigest handles all of this.
Convert to use the new The new TDigest provides better accuracy for quartile estimation as well as producing any other quantile you might like. The current quartile estimation of the fails for highly skewed distributions and can't really be extended to provide other quantiles. The TDigest handles all of this.
******
math.src.test.java.org.apache.mahout.math.stats.OnlineSummarizerTest.OnlineSummarizerTest.testStats(), false, test_method
math.src.test.java.org.apache.mahout.math.stats.OnlineSummarizerTest.OnlineSummarizerTest.normal(int), true
math.src.test.java.org.apache.mahout.math.stats.OnlineSummarizerTest.OnlineSummarizerTest.exp(int), true
math.src.test.java.org.apache.mahout.math.stats.OnlineSummarizerTest.OnlineSummarizerTest.gamma(int, double), false, new_method
math.src.main.java.org.apache.mahout.math.stats.OnlineSummarizer.OnlineSummarizer.add(double), true
math.src.main.java.org.apache.mahout.math.stats.OnlineSummarizer.OnlineSummarizer.sort(), true
math.src.main.java.org.apache.mahout.math.stats.OnlineSummarizer.OnlineSummarizer.getQuartile(int), true
mahout.examples.src.main.java.org.apache.mahout.clustering.streaming.tools.ClusterQualitySummarizer.ClusterQualitySummarizer.printSummaries(List<OnlineSummarizer>, String, PrintWriter), true
#####
mahout-0.8
MAHOUT-1367
https://issues.apache.org/jira/browse/MAHOUT-1367
WikipediaXmlSplitter --> Exception in thread "main" java.lang.NullPointerException Hi ! When I run this command : $MAHOUT_HOME/bin/mahout org.apache.mahout.text.wikipedia.WikipediaXmlSplitter -d frwiki-latest-pages-articles.xml.bz2 -o wikipedia-xml-chunks -c 100 I have this error : MAHOUT-JOB: /home/ollagnier/Documents/tools/mahout-distribution-0.8/bin/trunk/examples/target/mahout-examples-0.9-SNAPSHOT-job.jar 13/11/29 14:51:08 WARN driver.MahoutDriver: No org.apache.mahout.text.wikipedia.WikipediaXmlSplitter.props found on classpath, will use command-line arguments only 13/11/29 14:51:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Exception in thread "main" java.lang.NullPointerException at org.apache.hadoop.io.compress.bzip2.Bzip2Factory.isNativeBzip2Loaded(Bzip2Factory.java:54) at org.apache.hadoop.io.compress.bzip2.Bzip2Factory.getBzip2Decompressor(Bzip2Factory.java:131) at org.apache.hadoop.io.compress.BZip2Codec.createDecompressor(BZip2Codec.java:250) at org.apache.hadoop.io.compress.BZip2Codec.createInputStream(BZip2Codec.java:156) at org.apache.mahout.text.wikipedia.WikipediaXmlSplitter.main(WikipediaXmlSplitter.java:190) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72) at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144) at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:152) at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212) And I have a little trouble seeing where it comes from Thank you for your help
Exception in thread "main" Hi ! When I run this command  I have this error : will use command-line arguments only Unable to load native-hadoop library for your platform... using classes where applicable Exception in thread "main" And I have a little trouble seeing where it comes from Thank you for your help
******
integration.src.main.java.org.apache.mahout.text.wikipedia.WikipediaXmlSplitter.WikipediaXmlSplitter.main(String[]), true
#####
mahout-0.8
MAHOUT-1364
https://issues.apache.org/jira/browse/MAHOUT-1364
Upgrade Mahout codebase to Lucene 4.6 Parallel Randomized tests (using Carrot RandomizedRunner) fail on Mac OS for code that invokes Lucene API, see the discussion in M-1345. The fix is to upgrade to a Lucene version > 4.3.1 (which is the present Lucene version in Mahout trunk).
Upgrade Mahout codebase to Lucene 4.6 Parallel Randomized tests (using Carrot) fail on Mac OS for code that invokes Lucene API, see the discussion in M-1345. The fix is to upgrade to a Lucene version > 4.3.1 (which is the present Lucene version in Mahout trunk).
******
core.src.main.java.org.apache.mahout.vectorizer.document.SequenceFileTokenizerMapper.SequenceFileTokenizerMapper.map(Text, Text, Context), true
integration.src.test.java.org.apache.mahout.utils.vectors.lucene.LuceneIterableTest.LuceneIterableTest.createTestIndex(FieldType, RAMDirectory, int), false, test_method
integration.src.test.java.org.apache.mahout.text.LuceneSegmentInputSplitTest.LuceneSegmentInputSplitTest.assertSegmentContainsOneDoc(String), false, test_method
integration.src.test.java.org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest.CachedTermInfoTest.createTestIndex(FieldType, RAMDirectory, int), false, test_method
integration.src.main.java.org.apache.mahout.text.SequenceFilesFromLuceneStorageDriver.SequenceFilesFromLuceneStorageDriver.run(String[]), true
integration.src.test.java.org.apache.mahout.clustering.TestClusterDumper.TestClusterDumper.getSampleData(String[]), false, test_method
integration.src.main.java.org.apache.mahout.text.LuceneStorageConfiguration.LuceneStorageConfiguration.readFields(DataInput), true
integration.src.main.java.org.apache.mahout.text.SequenceFilesFromLuceneStorageMapper.SequenceFilesFromLuceneStorageMapper.setup(Context), true
integration.src.test.java.org.apache.mahout.text.TestSequenceFilesFromDirectory.TestSequenceFilesFromDirectory.testSequenceFileFromDirectoryMapReduce(), false, test_method
core.src.test.java.org.apache.mahout.vectorizer.encoders.TextValueEncoderTest.TextValueEncoderTest.testLuceneEncoding(), false, test_method
integration.src.main.java.org.apache.mahout.text.LuceneSegmentRecordReader.LuceneSegmentRecordReader.initialize(InputSplit, TaskAttemptContext), true
core.src.main.java.org.apache.mahout.common.lucene.AnalyzerUtils.AnalyzerUtils.createAnalyzer(String), true
integration.src.main.java.org.apache.mahout.utils.regex.AnalyzerTransformer.AnalyzerTransformer.AnalyzerTransformer(), true
integration.src.main.java.org.apache.mahout.text.wikipedia.WikipediaAnalyzer.WikipediaAnalyzer.WikipediaAnalyzer(), true
integration.src.main.java.org.apache.mahout.text.wikipedia.WikipediaAnalyzer.WikipediaAnalyzer.WikipediaAnalyzer(CharArraySet), true
integration.src.main.java.org.apache.mahout.text.wikipedia.WikipediaAnalyzer.WikipediaAnalyzer.createComponents(String, Reader), true
integration.src.test.java.org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest.BloomTokenFilterTest.testAnalyzer(), false, test_method
integration.src.test.java.org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest.BloomTokenFilterTest.testNonKeepdAnalyzer(), false, test_method
integration.src.test.java.org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest.BloomTokenFilterTest.testKeepAnalyzer(), false, test_method
integration.src.test.java.org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest.BloomTokenFilterTest.testShingleFilteredAnalyzer(), false, test_method
core.src.main.java.org.apache.mahout.common.lucene.TokenStreamIterator.TokenStreamIterator.computeNext(), true
integration.src.main.java.org.apache.mahout.text.LuceneSegmentInputFormat.LuceneSegmentInputFormat.getSplits(JobContext), true
integration.src.main.java.org.apache.mahout.text.ReadOnlyFileSystemDirectory.ReadOnlyFileSystemDirectory.clearLock(String), false, new_method
integration.src.main.java.org.apache.mahout.text.ReadOnlyFileSystemDirectory.ReadOnlyFileSystemDirectory.setLockFactory(LockFactory), false, new_method
integration.src.main.java.org.apache.mahout.text.ReadOnlyFileSystemDirectory.ReadOnlyFileSystemDirectory.getLockFactory(), false, new_method
integration.src.main.java.org.apache.mahout.text.ReadOnlyFileSystemDirectory.ReadOnlyFileSystemDirectory.ReadOnlyFileSystemDirectory(FileSystem, Path, boolean, Configuration), false, comments
integration.src.main.java.org.apache.mahout.text.ReadOnlyFileSystemDirectory.ReadOnlyFileSystemDirectory.finalize(), true
integration.src.test.java.org.apache.mahout.text.AbstractLuceneStorageTest.AbstractLuceneStorageTest.commitDocuments(Directory, Iterable<SingleFieldDocument>), false, test_method
integration.src.test.java.org.apache.mahout.text.LuceneSegmentRecordReaderTest.LuceneSegmentRecordReaderTest.testKey(), false, test_method
integration.src.test.java.org.apache.mahout.text.LuceneSegmentRecordReaderTest.LuceneSegmentRecordReaderTest.testNonExistingIdField(), false, test_method
integration.src.test.java.org.apache.mahout.text.LuceneSegmentRecordReaderTest.LuceneSegmentRecordReaderTest.testNonExistingField(), false, test_method
integration.src.main.java.org.apache.mahout.text.LuceneSegmentInputSplit.LuceneSegmentInputSplit.getSegment(Configuration), true
integration.src.test.java.org.apache.mahout.utils.vectors.lucene.DriverTest.DriverTest.sequenceFileDictionary(), false, test_method
#####
mahout-0.8
MAHOUT-1358
https://issues.apache.org/jira/browse/MAHOUT-1358
StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error java.lang.IllegalArgumentException: Must have nonzero number of training and test vectors. Asked for %.1f %% of %d vectors for test [10.000000149011612, 0] at com.google.common.base.Preconditions.checkArgument(Preconditions.java:120) at org.apache.mahout.clustering.streaming.cluster.BallKMeans.splitTrainTest(BallKMeans.java:176) at org.apache.mahout.clustering.streaming.cluster.BallKMeans.cluster(BallKMeans.java:192) at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.getBestCentroids(StreamingKMeansReducer.java:107) at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:73) at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:37) at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:177) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398) The issue is caused by the following code in StreamingKMeansThread.call() Iterator<Centroid> datapointsIterator = datapoints.iterator(); if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) { List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS); while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) { estimatePoints.add(datapointsIterator.next()); } estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure()); } StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff); while (datapointsIterator.hasNext()) { clusterer.cluster(datapointsIterator.next()); } The code is using the same iterator twice, and it fails on the second use for obvious reasons.
throws when is set to truethrows when is set to true Running  Clustering with and when no is specified, throws the following error The issue is caused by the following code in The code is using the same iterator twice, and it fails on the second use for obvious reasons.
******
core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.StreamingKMeansThread.StreamingKMeansThread(Iterable<Centroid>, Configuration), false, refactoring
core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.StreamingKMeansThread.call(), true
#####
mahout-0.8
MAHOUT-1357
https://issues.apache.org/jira/browse/MAHOUT-1357
InteractionValueEncoder produces wrong traceDictionary entries In the trace code the byte values of the terms being hashed are not converted back to string but just concatenated in their raw form with Arrays.asString() This makes the reverse engineering even harder! Fix is to just create new string, see patch attached.
produces wrong entries In the trace code the byte values of the terms being hashed are not converted back to string but just concatenated in their raw form with This makes the reverse engineering even harder! Fix is to just create new string, see patch attached.
******
core.src.main.java.org.apache.mahout.vectorizer.encoders.InteractionValueEncoder.InteractionValueEncoder.addInteractionToVector(byte[], byte[], double, Vector), true
core.src.test.java.org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest.InteractionValueEncoderTest.testTraceDictionary(), false, new_method
#####
mahout-0.8
MAHOUT-1351
https://issues.apache.org/jira/browse/MAHOUT-1351
Adding DenseVector support to AbstractCluster This improvement reduces runtime by 80% when performing k-means clustering of Scale Invariant Feature Transform (SIFT) descriptors to derive visual words for computer vision. Unlike sparse document vectors, SIFT descriptors are dense. This improvement involves updating the org.apache.mahout.clustering.AbstractCluster(Vector point, int id2) constructor to use "point.clone()" instead of "new RandomAccessSparseVector(point)" for creating the centroid. Also added testKMeansSeqJobDenseVector() test for DenseVector processing.
support to This improvement reduces runtime by 80% when performing clustering of Scale Invariant Feature Transform (SIFT) descriptors to derive visual words for computer vision. Unlike sparse document vectors, SIFT descriptors are dense. This improvement involves updating the constructor to use instead of for creating the centroid. Also added test for processing.
******
core.src.main.java.org.apache.mahout.clustering.AbstractCluster.AbstractCluster.AbstractCluster(Vector, int), true
core.src.test.java.org.apache.mahout.clustering.kmeans.TestKmeansClustering.TestKmeansClustering.getPointsWritableDenseVector(double[][]), false, new_method
core.src.test.java.org.apache.mahout.clustering.kmeans.TestKmeansClustering.TestKmeansClustering.testKMeansSeqJob(), false, test_method
core.src.test.java.org.apache.mahout.clustering.kmeans.TestKmeansClustering.TestKmeansClustering.testKMeansSeqJobDenseVector(), false, new_method
#####
mahout-0.8
MAHOUT-1349
https://issues.apache.org/jira/browse/MAHOUT-1349
Clusterdumper/loadTermDictionary crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size? I'm not sure if I'm doing something wrong here, or if ClusterDumper does not support my (fairly simple) use case I had a repository of 500K documents, for which I generated the input vectors and a dictionary using some custom code (not seq2sparse etc). I hashed the features with max size 5M (because I didn't know how many features were in the dataset and wanted to minimize collisions). The kmeans ran fine and generate sensible looking results, but when I tried to run ClusterDumper I got the following error: #bash> bin/mahout clusterdump -dt sequencefile -d completed/5159bba4e4b0718d03c8cf79_/EmailContentAnalytics_dict_5159bba4e4b0718d03c8cf79/part-* -i test-kmeans/clusters-19 -b 10 -n 10 -sp 10 -o ~/test-kmeans-out Running on hadoop, using /usr/bin/hadoop and HADOOP_CONF_DIR= MAHOUT-JOB: /opt/mahout-distribution-0.7/mahout-examples-0.7-job.jar 13/05/17 08:26:41 INFO common.AbstractJob: Command line arguments: {--dictionary=[completed/5159bba4e4b0718d03c8cf79_/EmailContentAnalytics_dict_5159bba4e4b0718d03c8cf79/part-*], --dictionaryType=[sequencefile], --distanceMeasure=[org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure], --endPhase=[2147483647], --input=[test-kmeans/clusters-19], --numWords=[10], --output=[/usr/share/tomcat6/test-kmeans-out], --outputFormat=[TEXT], --samplePoints=[10], --startPhase=[0], --substring=[10], --tempDir=[temp]} Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 698948 at org.apache.mahout.clustering.AbstractCluster.formatVector(AbstractCluster.java:350) at org.apache.mahout.clustering.AbstractCluster.asFormatString(AbstractCluster.java:306) at org.apache.mahout.utils.clustering.ClusterDumperWriter.write(ClusterDumperWriter.java:54) at org.apache.mahout.utils.clustering.AbstractClusterWriter.write(AbstractClusterWriter.java:169) at org.apache.mahout.utils.clustering.AbstractClusterWriter.write(AbstractClusterWriter.java:156) at org.apache.mahout.utils.clustering.ClusterDumper.printClusters(ClusterDumper.java:187) at org.apache.mahout.utils.clustering.ClusterDumper.run(ClusterDumper.java:153) (...) The error is when it tries to access the dictionary for the feature with index 698948 Looking at the dictionary loading code ( http://grepcode.com/file/repo1.maven.org/maven2/org.apache.mahout/mahout-integration/0.7/org/apache/mahout/utils/vectors/VectorHelper.java#VectorHelper.loadTermDictionary%28java.io.File%29 checked 0.8 and it hasn't changed) It looks like the dictionary array is sized for the number of unique keywords, not the highest index: OpenObjectIntHashMap dict = new OpenObjectIntHashMap(); //... String [] dictionary = new String[dict.size()]; After I ran my custom dictionary/feature generation code I discovered I only had 517,327 unique features, therefore it is not surprising it would die on an index >= 517327 (though I don't understand why it didn't die when trying to load the dictionary file) Is there any reason why the VectorHelper code should not create a dictionary array that has size the highest index read from the dictionary sequence file (which can be easily calculated during the preceding loop)? Or am I misunderstanding something? It worked fine when I reduced the hash size to be <= than the total number of features, but this is not desirable in general (for me) since I don't know the number of features before I run the job (and if I guess too high then ClusterDumper crashes) Alex Piggott IKANOW
crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size? I'm not sure if I'm doing something wrong here, or if does not support my (fairly simple) use case I had a repository of 500K documents, for which I generated the input vectors and a dictionary using some custom code (not etc). I hashed the features with max size 5M (because I didn't know how many features were in the dataset and wanted to minimize collisions). The ran fine and generate sensible looking results, but when I tried to run I got the following error: The error is when it tries to access the dictionary for the feature with index 698948 Looking at the dictionary loading code ( http://grepcode.com/file/repo1.maven.org/maven2/org.apache.mahout/mahout-integration/0.7/org/apache/mahout/utils/vectors/VectorHelper.java#VectorHelper.loadTermDictionary%28java.io.File%29 checked 0.8 and it hasn't changed) It looks like the dictionary array is sized for the number of unique keywords, not the highest index: After I ran my custom dictionary/feature generation code I discovered I only had 517,327 unique features, therefore it is not surprising it would die on an index >= 517327 (though I don't understand why it didn't die when trying to load the dictionary file) Is there any reason why the code should not create a dictionary array that has size the highest index read from the dictionary sequence file (which can be easily calculated during the preceding loop)? Or am I misunderstanding something? It worked fine when I reduced the hash size to be <= than the total number of features, but this is not desirable in general (for me) since I don't know the number of features before I run the job (and if I guess too high then crashes) Alex Piggott IKANOW
******
integration.src.test.java.org.apache.mahout.utils.vectors.VectorHelperTest.VectorHelperTest.setUp(), false, new_method
integration.src.test.java.org.apache.mahout.utils.vectors.VectorHelperTest.VectorHelperTest.testLoadTermDictionary(), false, new_method
integration.src.main.java.org.apache.mahout.utils.vectors.VectorHelper.VectorHelper.loadTermDictionary(Configuration, String), true
#####
mahout-0.8
MAHOUT-1339
https://issues.apache.org/jira/browse/MAHOUT-1339
Add RAT check; fix copyright headers Many files in the project don't carry the standard license header. There is a plugin called RAT that automates these checks. The attached patch adds a suitable RAT config, and fixes the errors it identifies. (I'm not yet proposing making the check part of the build.)
=== copyright headers. No source code changes ===
******
#####
mahout-0.8
MAHOUT-1336
https://issues.apache.org/jira/browse/MAHOUT-1336
HighDFWordsPrunerTest is failing silently Apparently ToolRunner does not allow the --mapred option. The validation is not very foolproof, so there is a resulting silent failure in HighDFWordsPrunerTest. Error message: org.apache.commons.cli2.OptionException: Unexpected --mapred while processing Options at org.apache.commons.cli2.commandline.Parser.parse(Parser.java:99) at org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.run(SparseVectorsFromSequenceFiles.java:154) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65) at org.apache.mahout.vectorizer.HighDFWordsPrunerTest.runTest(HighDFWordsPrunerTest.java:111) at org.apache.mahout.vectorizer.HighDFWordsPrunerTest.testHighDFWordsPruning(HighDFWordsPrunerTest.java:85) ... Usage: [--minSupport <minSupport> --analyzerName <analyzerName> --chunkSize <chunkSize> --output <output> --input <input> --minDF <minDF> --maxDFSigma <maxDFSigma> --maxDFPercent <maxDFPercent> --weight <weight> --norm <norm> --minLLR <minLLR> --numReducers <numReducers> --maxNGramSize <ngramSize> --overwrite --help --sequentialAccessVector --namedVector --logNormalize] O
is failing silently Apparentl does not allow the --mapred option. The validation is not very foolproof, so there is a resulting silent failure in. Error message:
******
core.src.test.java.org.apache.mahout.vectorizer.HighDFWordsPrunerTest.HighDFWordsPrunerTest.runTest(boolean), false, test_method
core.src.test.java.org.apache.mahout.vectorizer.HighDFWordsPrunerTest.HighDFWordsPrunerTest.alidateVectors(Path, int[], boolean), true
#####
mahout-0.8
MAHOUT-1320
https://issues.apache.org/jira/browse/MAHOUT-1320
BallKMeansTest.testClustering is unstable From time to time this test fails with following in build log: Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 48.134 sec <<< FAILURE! - in org.apache.mahout.clustering.streaming.cluster.BallKMeansTest testClustering(org.apache.mahout.clustering.streaming.cluster.BallKMeansTest) Time elapsed: 2.051 sec <<< FAILURE! java.lang.AssertionError: expected:<625.0> but was:<796.0> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:743) at org.junit.Assert.assertEquals(Assert.java:494) at org.junit.Assert.assertEquals(Assert.java:592) at org.apache.mahout.clustering.streaming.cluster.BallKMeansTest.testClustering(BallKMeansTest.java:119) Here is a bit more of build log output, which also shows other tests were running in parallel with this one: [INFO] --- maven-surefire-plugin:2.15:test (default-test) @ mahout-core --- [INFO] Surefire report directory: /home/jenkins/jenkins-slave/workspace/Mahout-Quality/trunk/core/target/surefire-reports [INFO] parallel='classes', perCoreThreadCount=false, threadCount=1, useUnlimitedThreads=false ------------------------------------------------------- T E S T S ------------------------------------------------------- ------------------------------------------------------- T E S T S ------------------------------------------------------- Running org.apache.mahout.common.distance.TestChebyshevMeasure Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.043 sec - in org.apache.mahout.common.distance.TestChebyshevMeasure Running org.apache.mahout.common.distance.TestMinkowskiMeasure Running org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure Running org.apache.mahout.common.distance.TestManhattanDistanceMeasure Running org.apache.mahout.common.distance.CosineDistanceMeasureTest Running org.apache.mahout.common.distance.TestTanimotoDistanceMeasure Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.143 sec - in org.apache.mahout.common.distance.TestMinkowskiMeasure Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.078 sec - in org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure Running org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.099 sec - in org.apache.mahout.common.distance.TestManhattanDistanceMeasure Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.075 sec - in org.apache.mahout.common.distance.CosineDistanceMeasureTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.094 sec - in org.apache.mahout.common.distance.TestTanimotoDistanceMeasure Running org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest Running org.apache.mahout.common.distance.TestEuclideanDistanceMeasure Running org.apache.mahout.common.iterator.TestFixedSizeSampler Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.135 sec - in org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure Running org.apache.mahout.common.iterator.CountingIteratorTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.009 sec - in org.apache.mahout.common.iterator.CountingIteratorTest Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.073 sec - in org.apache.mahout.common.iterator.TestFixedSizeSampler Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.111 sec - in org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.121 sec - in org.apache.mahout.common.distance.TestEuclideanDistanceMeasure Running org.apache.mahout.common.iterator.TestSamplingIterator Running org.apache.mahout.common.iterator.TestStableFixedSizeSampler Running org.apache.mahout.common.DummyRecordWriterTest Running org.apache.mahout.common.StringUtilsTest Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.093 sec - in org.apache.mahout.common.iterator.TestStableFixedSizeSampler Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.113 sec - in org.apache.mahout.common.DummyRecordWriterTest Running org.apache.mahout.common.AbstractJobTest Running org.apache.mahout.common.IntPairWritableTest Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.02 sec - in org.apache.mahout.common.IntPairWritableTest Running org.apache.mahout.common.lucene.AnalyzerUtilsTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.07 sec - in org.apache.mahout.common.lucene.AnalyzerUtilsTest Running org.apache.mahout.clustering.topdown.PathDirectoryTest Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in org.apache.mahout.clustering.topdown.PathDirectoryTest Running org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.505 sec - in org.apache.mahout.common.StringUtilsTest Running org.apache.mahout.clustering.classify.ClusterClassificationDriverTest Running org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest Running org.apache.mahout.clustering.spectral.TestVectorCache Running org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob Running org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob Running org.apache.mahout.clustering.lda.cvb.TestCVBModelTrainer Running org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob Running org.apache.mahout.clustering.spectral.TestUnitVectorizerJob Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.356 sec - in org.apache.mahout.common.AbstractJobTest Running org.apache.mahout.clustering.canopy.TestCanopyCreation Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.046 sec - in org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.288 sec - in org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.624 sec - in org.apache.mahout.clustering.spectral.TestVectorCache Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.713 sec - in org.apache.mahout.common.iterator.TestSamplingIterator Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.054 sec - in org.apache.mahout.clustering.spectral.TestUnitVectorizerJob Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.518 sec - in org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob Running org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.609 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest Running org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator Running org.apache.mahout.clustering.kmeans.TestKmeansClustering Running org.apache.mahout.clustering.TestGaussianAccumulators Running org.apache.mahout.clustering.iterator.TestClusterClassifier Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.065 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest Running org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering Running org.apache.mahout.clustering.TestClusterInterface Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.203 sec - in org.apache.mahout.clustering.TestClusterInterface Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.746 sec - in org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.897 sec - in org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator Running org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest Running org.apache.mahout.clustering.streaming.cluster.BallKMeansTest Running org.apache.mahout.math.stats.OnlineAucTest Running org.apache.mahout.math.stats.SamplerTest Running org.apache.mahout.math.VarintTest Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.031 sec - in org.apache.mahout.math.VarintTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.294 sec - in org.apache.mahout.math.stats.SamplerTest Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.821 sec - in org.apache.mahout.clustering.classify.ClusterClassificationDriverTest Running org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest Running org.apache.mahout.math.hadoop.stats.BasicStatsTest Running org.apache.mahout.math.hadoop.TestDistributedRowMatrix Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.175 sec - in org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest Running org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.366 sec - in org.apache.mahout.clustering.TestGaussianAccumulators Running org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.578 sec - in org.apache.mahout.math.stats.OnlineAucTest Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.893 sec - in org.apache.mahout.clustering.iterator.TestClusterClassifier Running org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest Running org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI Tests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.419 sec - in org.apache.mahout.clustering.canopy.TestCanopyCreation Running org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.33 sec - in org.apache.mahout.math.hadoop.stats.BasicStatsTest Running org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.475 sec - in org.apache.mahout.clustering.kmeans.TestKmeansClustering Running org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.076 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest Running org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.855 sec - in org.apache.mahout.math.hadoop.TestDistributedRowMatrix Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.63 sec - in org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob Running org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI Running org.apache.mahout.math.VectorWritableTest Tests run: 100, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.046 sec - in org.apache.mahout.math.VectorWritableTest Running org.apache.mahout.math.ssvd.SequentialOutOfCoreSvdTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.541 sec - in org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.658 sec - in org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver Running org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearchTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.045 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest Running org.apache.mahout.math.neighborhood.SearchSanityTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.918 sec - in org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering Running org.apache.mahout.math.MatrixWritableTest Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.944 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.08 sec - in org.apache.mahout.math.MatrixWritableTest Running org.apache.mahout.vectorizer.DocumentProcessorTest Running org.apache.mahout.vectorizer.HighDFWordsPrunerTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.631 sec - in org.apache.mahout.vectorizer.HighDFWordsPrunerTest Running org.apache.mahout.math.neighborhood.SearchQualityTest Running org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.062 sec - in org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.581 sec - in org.apache.mahout.vectorizer.DocumentProcessorTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.304 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest Running org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest Running org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.113 sec - in org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.137 sec - in org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest Running org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.174 sec - in org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest Running org.apache.mahout.vectorizer.encoders.TextValueEncoderTest Running org.apache.mahout.vectorizer.SparseVectorsFromSequenceFilesTest Running org.apache.mahout.vectorizer.collocations.llr.GramTest Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.082 sec - in org.apache.mahout.vectorizer.collocations.llr.GramTest Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.544 sec - in org.apache.mahout.vectorizer.encoders.TextValueEncoderTest Running org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.069 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest Running org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.061 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest Running org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest Running org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.552 sec - in org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.369 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest Running org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest Running org.apache.mahout.vectorizer.collocations.llr.GramKeyTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.081 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyTest Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.693 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest Running org.apache.mahout.vectorizer.DictionaryVectorizerTest Running org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 48.134 sec <<< FAILURE! - in org.apache.mahout.clustering.streaming.cluster.BallKMeansTest testClustering(org.apache.mahout.clustering.streaming.cluster.BallKMeansTest) Time elapsed: 2.051 sec <<< FAILURE! java.lang.AssertionError: expected:<625.0> but was:<796.0> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:743) at org.junit.Assert.assertEquals(Assert.java:494) at org.junit.Assert.assertEquals(Assert.java:592) at org.apache.mahout.clustering.streaming.cluster.BallKMeansTest.testClustering(BallKMeansTest.java:119) Last time test failed it was on ubuntu-1 node, but it's also randomly successful on same node so it doesn't seem to be caused by something node specific.
is unstable From time to time this test fails with following in build log: Here is a bit more of build log output, which also shows other tests were running in parallel with this one: Last time test failed it was on ubuntu-1 node, but it's also randomly successful on same node so it doesn't seem to be caused by something node specific.
******
core.src.test.java.org.apache.mahout.clustering.streaming.cluster.DataUtils.DataUtils.sampleMultiNormalHypercube(int, int, double), false, comments
core.src.test.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansTestMR.StreamingKMeansTestMR.setUp(), false, new_method
core.src.test.java.org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest.StreamingKMeansTest.setUp(), false, new_method
core.src.test.java.org.apache.mahout.clustering.streaming.cluster.BallKMeansTest.BallKMeansTest.setUp(), false, new_method
core.src.test.java.org.apache.mahout.clustering.streaming.cluster.BallKMeansTest.BallKMeansTest.testClustering(), false, test_method
#####
mahout-0.8
MAHOUT-1319
https://issues.apache.org/jira/browse/MAHOUT-1319
seqdirectory -filter argument silently ignored when run as MR Running "seqdirectory" (Sequence Files from Input Directory) from the command line and specifying a custom filter using the -filter parameter, the argument is ignored and the default "PrefixAdditionFilter" is used on the input. No exception is thrown. When the same command is run with "-xm sequential", the filter is found and works as expected.
seqdirectory -filter argument silently ignored when run as MR Running "seqdirectory" (Sequence Files from Input Directory) from the command line and specifying a custom filter using the -filter parameter, the argument is ignored and the default is used on the input. No exception is thrown. When the same command is run with "-xm sequential", the filter is found and works as expected.
******
integration.src.main.java.org.apache.mahout.text.SequenceFilesFromDirectory.SequenceFilesFromDirectory.runMapReduce(Path, Path), true
integration.src.main.java.org.apache.mahout.text.WholeFileRecordReader.WholeFileRecordReader.WholeFileRecordReader(CombineFileSplit, TaskAttemptContext, Integer), true
integration.src.main.java.org.apache.mahout.text.WholeFileRecordReader.WholeFileRecordReader.initialize(InputSplit, TaskAttemptContext), true
integration.src.main.java.org.apache.mahout.text.WholeFileRecordReader.WholeFileRecordReader.nextKeyValue(), true
integration.src.test.java.org.apache.mahout.text.TestSequenceFilesFromDirectory.TestSequenceFilesFromDirectory.testSequenceFileFromDirectoryMapReduce(), false, test_method
integration.src.test.java.org.apache.mahout.text.TestSequenceFilesFromDirectory.TestSequenceFilesFromDirectory.checkChunkFiles(Configuration, Path, String[][], String), true
integration.src.test.java.org.apache.mahout.text.TestSequenceFilesFromDirectory.TestSequenceFilesFromDirectory.accept(Path), true
integration.src.test.java.org.apache.mahout.text.TestSequenceFilesFromDirectory.TestSequenceFilesFromDirectory.checkRecursiveChunkFiles(Configuration, Path, String[][], String), true
integration.src.test.java.org.apache.mahout.text.TestSequenceFilesFromDirectory.TestSequenceFilesFromDirectory.heckMRResultFiles(Configuration, Path, String[][], String), true
integration.src.test.java.org.apache.mahout.text.TestSequenceFilesFromDirectory.TestSequenceFilesFromDirectory.checkMRResultFilesRecursive(Configuration, Path, String[][], String), true
integration.src.test.java.org.apache.mahout.text.TestPathFilter.TestPathFilter.accept(Path), false, new_method
core.src.main.java.org.apache.mahout.common.HadoopUtil.HadoopUtil.buildDirList(FileSystem, FileStatus), true
core.src.main.java.org.apache.mahout.common.HadoopUtil.HadoopUtil.buildDirList(FileSystem, FileStatus, PathFilter), false, new_method
#####
mahout-0.8
MAHOUT-1317
https://issues.apache.org/jira/browse/MAHOUT-1317
Clarify some of the messages in Preconditions.checkArgument In experimenting with things, I was getting some errors from RowSimilarityJob, that in looking at the source I realized were a little incomplete as to what the true issue was. In this case, they were of the form: Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Incorrect maximum number of similarities per row!"); Here, it is known that the actual issue is that the parameter must be zero (or negative), not just that it's "incorrect", and a (trivial) change to the error message might save some folks some time... especially newbies like myself. A quick grep of the code showed a few more cases like that across the code base that would be (apparently) easy to fix and maybe save folks time when they get the relevant error.
Clarify some of the messages in In experimenting with things, I was getting some errors from that in looking at the source I realized were a little incomplete as to what the true issue was. In this case, they were of the form: "Incorrect maximum number of similarities per row!"); Here, it is known that the actual issue is that the parameter must be zero (or negative), not just that it's "incorrect", and a (trivial) change to the error message might save some folks some time... especially newbies like myself. A quick grep of the code showed a few more cases like that across the code base that would be (apparently) easy to fix and maybe save folks time when they get the relevant error.
******
core.src.main.java.org.apache.mahout.classifier.df.mapreduce.partial.Step1Mapper.Step1Mapper.configure(Long, int, int, int), true
core.src.main.java.org.apache.mahout.clustering.kmeans.RandomSeedGenerator.RandomSeedGenerator.buildRandom(Configuration, Path, Path, int, DistanceMeasure), true
core.src.main.java.org.apache.mahout.classifier.df.data.Dataset.Dataset.valueOf(int, String), true
core.src.main.java.org.apache.mahout.cf.taste.hadoop.als.SolveImplicitFeedbackMapper.SolveImplicitFeedbackMapper.createSharedInstance(Context), false, refactoring
core.src.main.java.org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluator.GenericRecommenderIRStatsEvaluator.evaluate(RecommenderBuilder, DataModelBuilder, DataModel, IDRescorer, int, double, double), true
core.src.main.java.org.apache.mahout.math.neighborhood.BruteSearch.BruteSearch.search(Vector, int), true
core.src.main.java.org.apache.mahout.cf.taste.hadoop.als.SolveExplicitFeedbackMapper.SolveExplicitFeedbackMapper.setup(Mapper.Context), false, refactoring
core.src.main.java.org.apache.mahout.cf.taste.impl.recommender.SamplingCandidateItemsStrategy.SamplingCandidateItemsStrategy.SamplingCandidateItemsStrategy(int, int, int, int, int), true
core.src.main.java.org.apache.mahout.cf.taste.impl.common.WeightedRunningAverage.WeightedRunningAverage.changeDatum(double, double), true
core.src.main.java.org.apache.mahout.classifier.df.data.DataConverter.DataConverter.convert(CharSequence), true
core.src.main.java.org.apache.mahout.classifier.df.data.DataLoader.DataLoader.parseString(Attribute[], Set<String>[], CharSequence, boolean), true
core.src.main.java.org.apache.mahout.classifier.df.data.DataLoader.DataLoader.loadData(Dataset, FileSystem, Path), true
core.src.main.java.org.apache.mahout.classifier.df.data.DataLoader.DataLoader.loadData(Dataset, String[]), true
core.src.main.java.org.apache.mahout.classifier.df.data.DataLoader.DataLoader.generateDataset(CharSequence, boolean, FileSystem, Path), true
core.src.main.java.org.apache.mahout.classifier.df.data.DataLoader.DataLoader.generateDataset(CharSequence, boolean, String[]), true
integration.src.main.java.org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel.MongoDBDataModel.checkData(String, Iterable<List<String>>, boolean), true
math.src.main.java.org.apache.mahout.math.als.AlternatingLeastSquaresSolver.AlternatingLeastSquaresSolver.solve(Iterable<Vector>, Vector, double, int), true
math.src.main.java.org.apache.mahout.math.als.AlternatingLeastSquaresSolver.AlternatingLeastSquaresSolver.addLambdaTimesNuiTimesE(Matrix, double, int), true
math.src.main.java.org.apache.mahout.math.als.AlternatingLeastSquaresSolver.AlternatingLeastSquaresSolver.createRiIiMaybeTransposed(Vector), true
examples.src.main.java.org.apache.mahout.cf.taste.example.kddcup.KDDCupDataModel.KDDCupDataModel.DDCupDataModel(File, boolean, double), true
integration.src.main.java.org.apache.mahout.utils.vectors.lucene.LuceneIterator.LuceneIterator.LuceneIterator(IndexReader, String, String, TermInfo, Weight, double), true
integration.src.main.java.org.apache.mahout.utils.vectors.lucene.LuceneIterator.LuceneIterator.LuceneIterator(IndexReader, String, String, TermInfo, Weight, double, double), true
core.src.main.java.org.apache.mahout.cf.taste.impl.recommender.TopItems.TopItems.getTopItems(int, LongPrimitiveIterator, IDRescorer, Estimator<Long>), true
core.src.main.java.org.apache.mahout.cf.taste.impl.eval.AbstractDifferenceRecommenderEvaluator.AbstractDifferenceRecommenderEvaluator.evaluate(RecommenderBuilder, DataModelBuilder, DataModel, double, double), true
integration.src.main.java.org.apache.mahout.cf.taste.impl.model.cassandra.CassandraDataModel.CassandraDataModel.CassandraDataModel(String, int, String), true
math.src.main.java.org.apache.mahout.math.random.Empirical.Empirical.Empirical(boolean, boolean, int, double...), true
core.src.main.java.org.apache.mahout.cf.taste.impl.similarity.GenericUserSimilarity.GenericUserSimilarity.UserUserSimilarity(long, long, double), true
core.src.main.java.org.apache.mahout.classifier.naivebayes.training.WeightsMapper.WeightsMapper.setup(Context), true
core.src.main.java.org.apache.mahout.math.neighborhood.FastProjectionSearch.FastProjectionSearch.FastProjectionSearch(DistanceMeasure, int, int), true
math.src.main.java.org.apache.mahout.math.random.ChineseRestaurant.ChineseRestaurant.createRiIiMaybeTransposed(Vector), true
core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.StreamingKMeansDriver.configureOptionsForWorkers(Configuration, int, int, float, int, float, boolean, boolean, float, int, String, String, int, int, String, boolean), false, test_method
core.src.main.java.org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIterator.SamplingLongPrimitiveIterator.SamplingLongPrimitiveIterator(RandomWrapper, LongPrimitiveIterator, double), true
core.src.main.java.org.apache.mahout.classifier.df.mapreduce.partial.TreeID.TreeID.TreeID(int, int), true
core.src.main.java.org.apache.mahout.math.Varint.Varint.readUnsignedVarLong(DataInput), true
core.src.main.java.org.apache.mahout.common.iterator.SamplingIterator.SamplingIterator.SamplingIterator(RandomWrapper, Iterator<? extends T>, double), true
core.src.main.java.org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.RowSimilarityJob.setup(Context), true
core.src.main.java.org.apache.mahout.math.neighborhood.ProjectionSearch.ProjectionSearch.ProjectionSearch(DistanceMeasure, int, int), true
core.src.main.java.org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.ItemSimilarityJob.setup(Context), true
core.src.test.java.org.apache.mahout.math.neighborhood.SearchQualityTest.SearchQualityTest.apply(WeightedThing<Vector>), false, test_method
core.src.main.java.org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarity.GenericItemSimilarity.ItemItemSimilarity(long, long, double), true
core.src.main.java.org.apache.mahout.cf.taste.impl.eval.IRStatisticsImpl.IRStatisticsImpl.IRStatisticsImpl(double, double, double, double, double), true
org.apache.mahout.utils.SplitInput.SplitInput.validate(), true
org.apache.mahout.math.CholeskyDecomposition.CholeskyDecomposition.CholeskyDecomposition(Matrix, boolean), true
#####
mahout-0.8
MAHOUT-1314
https://issues.apache.org/jira/browse/MAHOUT-1314
StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException. the problem is in the reduce method itself: on line 60 ( return input.getCentroid(); ) it should be input.getCentroid().clone(); similar to line 81. full stack trace: java.lang.NullPointerException at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191) at org.apache.mahout.math.random.WeightedThing.<init>(WeightedThing.java:31) at org.apache.mahout.math.neighborhood.BruteSearch.searchFirst(BruteSearch.java:133) at org.apache.mahout.clustering.ClusteringUtils.estimateDistanceCutoff(ClusteringUtils.java:100) at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:64) at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:66) at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:1) at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:650) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260) it happens every time the REDUCE_STREAMING_KMEANS is set to true.
throws when is set to true when option is set to true (-rskm) the reducer fails with the problem is in the reduce method itself: on line 60 it should be similar to line 81. full stack trace: it happens every time the is set to true
******
core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.StreamingKMeansReducer.reduce(IntWritable, Iterable<CentroidWritable>, Context), true
core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.StreamingKMeansReducer.getBestCentroids(List<Centroid>, Configuration), true
#####
mahout-0.8
MAHOUT-1308
https://issues.apache.org/jira/browse/MAHOUT-1308
Cannot extend CandidateItemsStrategy due to restricted visibility In order to implement a custom CandidateItemsStrategy, I'd like to extend AbstractCandidateItemsStrategy. This is not possible as the visibility modifier of doGetCandidateItems() is package default. Can it be extended to protected?
Cannot extend due to restricted visibility In order to implement a custom I'd like to extend This is not possible as the visibility modifier of is package default. Can it be extended to protected?
******
core.src.main.java.org.apache.mahout.cf.taste.impl.recommender.AllSimilarItemsCandidateItemsStrategy.AllSimilarItemsCandidateItemsStrategy.doGetCandidateItems(long[], DataModel), false, access_modifier
#####
mahout-0.8
MAHOUT-1301
https://issues.apache.org/jira/browse/MAHOUT-1301
toString() method of SequentialAccessSparseVector has excess comma at the end Realization of SequentialAccessSparseVector toString() method had changed in MAHOUT-1259 patch. Unfortunately, that patch introduced new bug: output of the toString() method had been changed - extra comma added at the end of the string Example: Consider following sparse vector Vector v = new SequentialAccessSparseVector(capacity); v.set(1, 0.1); v.set(3, 0.3); In 0.7 v.toString() returns following string: {1:0.1,3:0.3} but in 0.8 it returns {1:0.1,3:0.3,} As you can see, there is extra comma at the end of the string.
method of has excess comma at the end Realization of method had changed in MAHOUT-1259 patch. Unfortunately, that patch introduced new bug: output of the method had been changed - extra comma added at the end of the string Example: Consider following sparse vector In returns following string: As you can see, there is extra comma at the end of the string.
******
math.src.main.java.org.apache.mahout.math.RandomAccessSparseVector.RandomAccessSparseVector.toString(), true
math.src.main.java.org.apache.mahout.math.AbstractVector.AbstractVector.sparseVectorToString(), false, new_method
math.src.test.java.org.apache.mahout.math.TestRandomAccessSparseVector.TestRandomAccessSparseVector.testToString(), false, new_method
math.src.test.java.org.apache.mahout.math.TestSequentialAccessSparseVector.TestSequentialAccessSparseVector.testToString(), false, new_method
math.src.test.java.org.apache.mahout.math.AbstractVectorTest.AbstractVectorTest.testIterators(), false, test_method
math.src.test.java.org.apache.mahout.math.AbstractVectorTest.AbstractVectorTest.testToString(), false, new_method
math.src.test.java.org.apache.mahout.math.TestDenseVector.TestDenseVector.testToString(), false, new_method
math.src.main.java.org.apache.mahout.math.SequentialAccessSparseVector.SequentialAccessSparseVector.toString(), true
org.apache.mahout.math.DenseVector.DenseVector.DenseVector(double[]), false, comments
org.apache.mahout.math.DenseVector.DenseVector.DenseVector(int), false, comments
#####
mahout-0.8
MAHOUT-1291
https://issues.apache.org/jira/browse/MAHOUT-1291
MahoutDriver yields cosmetically suboptimal exception when bin/mahout runs without args, on some Hadoop versions If you run bin/mahout without arguments, an error is correctly displayed about lack of an argument. The part that displays the error is actually within Hadoop code. In some versions of Hadoop, in the error case, it will quit the JVM with System.exit(). In others, it does not. In the calling code in MahoutDriver, in this error case, the main() method does not actually return. So, for versions where Hadoop code doesn't immediately exit the JVM, execution continues. This yields another exception. It's pretty harmless but ugly. Attached is a one-line fix, to return from main() in the error case, which is more correct to begin with.
yields cosmetically suboptimal exception when bin/mahout runs without args, on some Hadoop versions If you run bin/mahout without arguments, an error is correctly displayed about lack of an argument. The part that displays the error is actually within Hadoop code. In some versions of Hadoop, in the error case, it will quit the JVM with. In others, it does not. In the calling code in this error case, the main() method does not actually return. So, for versions where Hadoop code doesn't immediately exit the JVM, execution continues. This yields another exception. It's pretty harmless but ugly. Attached is a one-line fix, to return from main() in the error case, which is more correct to begin with.
******
core.src.main.java.org.apache.mahout.driver.MahoutDriver.MahoutDriver.main(String[]), true
#####
mahout-0.8
MAHOUT-1285
https://issues.apache.org/jira/browse/MAHOUT-1285
Arff loader can misparse string data as double Have successfully loaded numerous ARFF files with Mahout (originally generated via WEKA). The files contain randomly generated data. For a specific random seed, the following exception is thrown: java.lang.NumberFormatException: For input string: "b1shkt70694difsmmmdv0ikmoh" at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1241) at java.lang.Double.parseDouble(Double.java:540) at org.apache.mahout.utils.vectors.arff.MapBackedARFFModel.processNumeric(MapBackedARFFModel.java:146) at org.apache.mahout.utils.vectors.arff.MapBackedARFFModel.getValue(MapBackedARFFModel.java:97) at org.apache.mahout.utils.vectors.arff.ARFFIterator.computeNext(ARFFIterator.java:77) at org.apache.mahout.utils.vectors.arff.ARFFIterator.computeNext(ARFFIterator.java:30) at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143) at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138) at org.apache.mahout.utils.vectors.io.SequenceFileVectorWriter.write(SequenceFileVectorWriter.java:44) at org.apache.mahout.utils.vectors.arff.Driver.writeFile(Driver.java:251) at org.apache.mahout.utils.vectors.arff.Driver.main(Driver.java:145) at libInterfaces.MahoutTraceBuilder.generateMahoutFile(MahoutTraceBuilder.java:38) at libInterfaces.MahoutTraceBuilder.generateMahoutReader(MahoutTraceBuilder.java:42) at tests.InputTester.testMahoutMeansShift(InputTester.java:111)
loader can misparse string data as double Have successfully loaded numerous files with Mahout (originally generated via WEKA). The files contain randomly generated data. For a specific random seed, the following exception is thrown:
******
integration.src.main.java.org.apache.mahout.utils.vectors.arff.ARFFVectorIterable.ARFFVectorIterable.ARFFVectorIterable(Reader, ARFFModel), true
integration.src.main.java.org.apache.mahout.utils.vectors.arff.MapBackedARFFModel.MapBackedARFFModel.processNumeric(String), true
integration.src.main.java.org.apache.mahout.utils.vectors.arff.MapBackedARFFModel.MapBackedARFFModel.isNumeric(String), false, new_method
org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest.MapBackedARFFModelTest.processBadNumeric(), false, test_method
org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest.MapBackedARFFModelTest.processGoodNumeric(), false, test_method
#####
mahout-0.8
MAHOUT-1284
https://issues.apache.org/jira/browse/MAHOUT-1284
DummyRecordWriter's bug with reused Writables It is a recommended practice to reuse the Writable objects. DummyRecordWriter, which is used for testing in Mahout, however keeps the same Writable instance in a map: next time that the user reuses the Writable object, the internal map of DummyRecordWriter changes as well. This makes DummyRecordWriter fail for testing the MapReduce jobs that reuse the Writables.
bug with reused It is a recommended practice to reuse the objects. which is used for testing in Mahout, however keeps the same instance in a map: next time that the user reuses the object, the internal map of changes as well. This makes fail for testing the jobs that reuse the.
******
core.src.test.java.org.apache.mahout.common.DummyRecordWriter.DummyRecordWriter.write(K, V), false, test_method
core.src.test.java.org.apache.mahout.common.DummyRecordWriter.DummyRecordWriter.cloneWritable(Writable, Writable), false, new_method
core.src.test.java.org.apache.mahout.common.DummyRecordWriterTest.DummyRecordWriterTest.testWrite(), false, new_method
#####
mahout-0.8
MAHOUT-1276
https://issues.apache.org/jira/browse/MAHOUT-1276
job name for ParallelALSFactorizationJob is confusing ParallelALSFactorizationJob set job name per each iteration in different with logger`s info. job name start from 2 and numIterations is 0 based. therefore if I run job with iteration 5, job names start from "Recompute M, iteration (2/5)". It would be better if we set job name same as logger info message since current job name could be confusing.
job name for is confusing set job name per each iteration in different with logger`s info. job name start from 2 and is 0 based. therefore if I run job with iteration 5, job names start from . It would be better if we set job name same as logger info message since current job name could be confusing.
******
core.src.main.java.org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.ParallelALSFactorizationJob.runSolver(Path, Path, Path, int, String, int), true
#####
mahout-0.8
MAHOUT-1261
https://issues.apache.org/jira/browse/MAHOUT-1261
TasteHadoopUtils.idToIndex can return an int that has size Integer.MAX_VALUE I'm running ItemSimilarityJob on a very large (~600M by 4B) matrix that's very sparse (total set of associations is 630MB). The job fails because of an IndexException in ToUserVectorsReducer. TasteHadoopUtils.idToIndex(long id) hashes a long with: 0x7fffffff & Longs.hashCode(id) (line o.a.m.cf.taste.hadoop.TasteHadoopUtils:57). For some id (I don't know what value), the result returned is Integer.MAX_VALUE. This cannot be set in the userVector because the cardinality of that is also Integer.MAX_VALUE and it throws an exception. So, the issue is that values from 0 to INT_MAX are returned by idToIndex but the vector only has 0 to INT_MAX - 1 possible entries. It's a nasty little off-by-one bug. I'm thinking of just % size when setting. Sebastian Schelter & everyone else, thoughts?
can return an int that has size I'm running on a very large (~600M by 4B) matrix that's very sparse (total set of associations is 630MB). The job fails because of an hashes a long with:. For some id (I don't know what value), the result returned is This cannot be set in the because the cardinality of that is also and it throws an exception. So, the issue is that values from 0 to INT_MAX are returned by but the vector only has 0 to INT_MAX - 1 possible entries. It's a nasty little off-by-one bug. I'm thinking of just % size when setting. Sebastian Schelter & everyone else, thoughts?
******
core.src.main.java.org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils.TasteHadoopUtils.idToIndex(long), true
core.src.test.java.org.apache.mahout.cf.taste.hadoop.TasteHadoopUtilsTest.TasteHadoopUtilsTest.testWithinRange(), false, new_method
#####
mahout-0.8
MAHOUT-1242
https://issues.apache.org/jira/browse/MAHOUT-1242
No key redistribution function for associative maps All integer-based maps currently use HashFunctions.hash(int) which just returns the key value: /** * Returns a hashcode for the specified value. * * @return a hash code value for the specified value. */ public static int hash(int value) { return value; //return value * 0x278DDE6D; // see org.apache.mahout.math.jet.random.engine.DRand /* value &= 0x7FFFFFFF; // make it >=0 int hashCode = 0; do hashCode = 31*hashCode + value%10; while ((value /= 10) > 0); return 28629151*hashCode; // spread even further; h*31^5 */ } This easily leads to very degenerate behavior on keys that have constant lower bits (long collision chains). A simple (and strong) hash function like the final step of murmurhash3 goes a long way at ensuring the keys distribution is more uniform regardless of the input distribution.
No key redistribution function for associative maps All integer-based maps currently use which just returns the key value: This easily leads to very degenerate behavior on keys that have constant lower bits (long collision chains). A simple (and strong) hash function like the final step of goes a long way at ensuring the keys distribution is more uniform regardless of the input distribution.
******
math.src.main.java.org.apache.mahout.math.map.HashFunctions.HashFunctions.hash(int), true
org.apache.mahout.math.TestRandomAccessSparseVector.TestRandomAccessSparseVector.testToString(), false, test_method
#####
mahout-0.8
MAHOUT-1030
https://issues.apache.org/jira/browse/MAHOUT-1030
Regression: Clustered Points Should be WeightedPropertyVectorWritable not WeightedVectorWritable Looks like this won't make it into this build. Pretty widespread impact on code and tests and I don't know which properties were implemented in the old version. I will create a JIRA and post my interim results. On 6/8/12 12:21 PM, Jeff Eastman wrote: > That's a reversion that evidently got in when the new ClusterClassificationDriver was introduced. It should be a pretty easy fix and I will see if I can make the change before Paritosh cuts the release bits tonight. > > On 6/7/12 1:00 PM, Pat Ferrel wrote: >> It appears that in kmeans the clusteredPoints are now written as WeightedVectorWritable where in mahout 0.6 they were WeightedPropertyVectorWritable? This means that the distance from the centroid is no longer stored here? Why? I hope I'm wrong because that is not a welcome change. How is one to order clustered docs by distance from cluster centroid? >> >> I'm sure I could calculate the distance but that would mean looking up the centroid for the cluster id given in the above WeightedVectorWritable, which means iterating through all the clusters for each clustered doc. In my case the number of clusters could be fairly large. >> >> Am I missing something? >> >> >
Regression: Clustered Points Should be not Looks like this won't make it into this build. Pretty widespread impact on code and tests and I don't know which properties were implemented in the old version. I will create a JIRA and post my interim results. On 6/8/12 12:21 PM, Jeff Eastman wrote: > That's a reversion that evidently got in when the new was introduced. It should be a pretty easy fix and I will see if I can make the change before Paritosh cuts the release bits tonight. > > On 6/7/12 1:00 PM, Pat Ferrel wrote: >> It appears that in the are now written as where in mahout 0.6 they were? This means that the distance from the centroid is no longer stored here? Why? I hope I'm wrong because that is not a welcome change. How is one to order clustered docs by distance from cluster centroid? >> >> I'm sure I could calculate the distance but that would mean looking up the centroid for the cluster id given in the above which means iterating through all the clusters for each clustered doc. In my case the number of clusters could be fairly large. >> >> Am I missing something? >> >> >
******
integration.src.main.java.org.apache.mahout.utils.clustering.ClusterDumper.ClusterDumper.getClusterIdToPoints(), false, method_signature
integration.src.main.java.org.apache.mahout.utils.clustering.ClusterDumper.ClusterDumper.readPoints(Path, long, Configuration), false, refactoring
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationDriver.ClusterClassificationDriver.selectCluster(Path, List<Cluster>, ClusterClassifier, Path, Double, boolean), false, refactoring
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationDriver.ClusterClassificationDriver.classifyAndWrite(List<Cluster>, Double, boolean, SequenceFile.Writer, VectorWritable, Vector), true
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationDriver.ClusterClassificationDriver.classifyClusterMR(Configuration, Path, Path, Path, Double, boolean), false, refactoring
integration.src.main.java.org.apache.mahout.utils.clustering.JsonClusterWriter.JsonClusterWriter.JsonClusterWriter(Writer, Map<Integer, List<WeightedVectorWritable>>, DistanceMeasure, int, String[]), false, refactoring
integration.src.main.java.org.apache.mahout.utils.clustering.GraphMLClusterWriter.GraphMLClusterWriter.GraphMLClusterWriter(Writer, Map<Integer, List<WeightedVectorWritable>>, DistanceMeasure, int, String[], int), false, refactoring
integration.src.main.java.org.apache.mahout.utils.clustering.GraphMLClusterWriter.GraphMLClusterWriter.write(ClusterWritable), false, refactoring
core.src.test.java.org.apache.mahout.clustering.classify.ClusterClassificationDriverTest.ClusterClassificationDriverTest.collectVectorsForAssertion(), false, refactoring
integration.src.main.java.org.apache.mahout.utils.clustering.CSVClusterWriter.CSVClusterWriter.write(ClusterWritable), false, refactoring
integration.src.main.java.org.apache.mahout.utils.clustering.CSVClusterWriter.CSVClusterWriter.CSVClusterWriter(Writer, Map<Integer, List<WeightedVectorWritable>>, DistanceMeasure), false, refactoring
integration.src.main.java.org.apache.mahout.utils.vectors.lucene.ClusterLabels.ClusterLabels.getLabels(), false, refactoring
integration.src.main.java.org.apache.mahout.utils.vectors.lucene.ClusterLabels.ClusterLabels.getClusterLabels(Integer, Collection<WeightedVectorWritable>), false, refactoring
integration.src.main.java.org.apache.mahout.utils.clustering.ClusterDumperWriter.ClusterDumperWriter.write(ClusterWritable), true
integration.src.main.java.org.apache.mahout.utils.clustering.ClusterDumperWriter.ClusterDumperWriter.ClusterDumperWriter(Writer, Map<Integer, List<WeightedVectorWritable>>, DistanceMeasure, int, String[], int), false, method_signature
integration.src.main.java.org.apache.mahout.utils.clustering.AbstractClusterWriter.AbstractClusterWriter.AbstractClusterWriter(Writer, Map<Integer, List<WeightedVectorWritable>>, DistanceMeasure), false, refactoring
integration.src.main.java.org.apache.mahout.utils.clustering.AbstractClusterWriter.AbstractClusterWriter.getClusterIdToPoints(), false, method_signature
core.src.test.java.org.apache.mahout.clustering.kmeans.TestKmeansClustering.TestKmeansClustering.testKMeansSeqJob(), false, refactoring
core.src.test.java.org.apache.mahout.clustering.kmeans.TestKmeansClustering.TestKmeansClustering.testKMeansSeqJobDenseVector(), false, refactoring
core.src.test.java.org.apache.mahout.clustering.kmeans.TestKmeansClustering.TestKmeansClustering.testKMeansMRJob(), false, refactoring
core.src.test.java.org.apache.mahout.clustering.kmeans.TestKmeansClustering.TestKmeansClustering.testKMeansWithCanopyClusterInput(), false, refactoring
core.src.main.java.org.apache.mahout.clustering.classify.ClusterClassificationMapper.ClusterClassificationMapper.write(VectorWritable, Context, int, double), true
#####
