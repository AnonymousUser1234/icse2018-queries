lucene-4.0
LUCENE-4561
https://issues.apache.org/jira/browse/LUCENE-4561
DWPT assert tripped again TestBagOfPositions tripped the spooky DWPT ram used on flush assert in http://jenkins.sd-datasolutions.de/job/Lucene-Solr-4.x-Linux/2472/ It reproduces for me: ant test -Dtestcase=TestBagOfPositions -Dtests.method=test -Dtests.seed=730E05D38A0E4AFA -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=de_DE_PREEURO -Dtests.timezone=America/Metlakatla -Dtests.file.encoding=UTF-8 Full failure: [junit4:junit4] 2> NOTE: reproduce with: ant test -Dtestcase=TestBagOfPositions -Dtests.method=test -Dtests.seed=730E05D38A0E4AFA -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=de_DE_PREEURO -Dtests.timezone=America/Metlakatla -Dtests.file.encoding=UTF-8 [junit4:junit4] ERROR 63.2s J0 | TestBagOfPositions.test <<< [junit4:junit4] > Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=2163, name=Thread-1670, state=RUNNABLE, group=TGRP-TestBagOfPositions] [junit4:junit4] > Caused by: java.lang.AssertionError: actual mem: 33763152 byte, expected mem: 33755888 byte, flush mem: 33610208, active mem: 152944, pending DWPT: 0, flushing DWPT: 2, blocked DWPT: 0, peakDelta mem: 67152 byte [junit4:junit4] > at __randomizedtesting.SeedInfo.seed([730E05D38A0E4AFA]:0) [junit4:junit4] > at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:120) [junit4:junit4] > at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:187) [junit4:junit4] > at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:384) [junit4:junit4] > at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1451) [junit4:junit4] > at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1126) [junit4:junit4] > at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:201) [junit4:junit4] > at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:160) [junit4:junit4] > at org.apache.lucene.index.TestBagOfPositions$1.run(TestBagOfPositions.java:111) [junit4:junit4] > Throwable #2: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=2164, name=Thread-1671, state=RUNNABLE, group=TGRP-TestBagOfPositions] [junit4:junit4] > Caused by: java.lang.AssertionError: actual mem: 33763152 byte, expected mem: 33755888 byte, flush mem: 33610208, active mem: 152944, pending DWPT: 0, flushing DWPT: 2, blocked DWPT: 0, peakDelta mem: 67152 byte [junit4:junit4] > at __randomizedtesting.SeedInfo.seed([730E05D38A0E4AFA]:0) [junit4:junit4] > at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:120) [junit4:junit4] > at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:187) [junit4:junit4] > at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:384) [junit4:junit4] > at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1451) [junit4:junit4] > at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1126) [junit4:junit4] > at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:201) [junit4:junit4] > at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:160) [junit4:junit4] > at org.apache.lucene.index.TestBagOfPositions$1.run(TestBagOfPositions.java:111) [junit4:junit4] > Throwable #3: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=2166, name=Thread-1673, state=RUNNABLE, group=TGRP-TestBagOfPositions] [junit4:junit4] > Caused by: java.lang.AssertionError: actual mem: 33763152 byte, expected mem: 33755888 byte, flush mem: 33610208, active mem: 152944, pending DWPT: 0, flushing DWPT: 2, blocked DWPT: 0, peakDelta mem: 67152 byte [junit4:junit4] > at __randomizedtesting.SeedInfo.seed([730E05D38A0E4AFA]:0) [junit4:junit4] > at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:120) [junit4:junit4] > at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:187) [junit4:junit4] > at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:384) [junit4:junit4] > at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1451) [junit4:junit4] > at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1126) [junit4:junit4] > at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:201) [junit4:junit4] > at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:160) [junit4:junit4] > at org.apache.lucene.index.TestBagOfPositions$1.run(TestBagOfPositions.java:111) [junit4:junit4] 2> NOTE: test params are: codec=SimpleText, sim=RandomSimilarityProvider(queryNorm=false,coord=no): {field=DFR I(ne)1}, locale=de_DE_PREEURO, timezone=America/Metlakatla [junit4:junit4] 2> NOTE: Linux 3.2.0-32-generic amd64/IBM Corporation 1.6.0 (64-bit)/cpus=8,threads=1,free=55064736,total=63793152 [junit4:junit4] 2> NOTE: All tests run in this JVM: [Nested1, TestIndexWriterWithThreads, TestLevenshteinAutomata, TestBasicOperations, TestStressNRT, TestIndexWriterExceptions, TestConjunctions, TestMultiLevelSkipList, TestConcurrentMergeScheduler, TestIndexWriterOnDiskFull, TestBytesRef, TestThreadedForceMerge, TestDocument, TestCopyBytes, TestIndexWriterNRTIsCurrent, TestScoreCachingWrappingScorer, TestScorerPerf, TestDocValuesTypeCompatibility, TestPrefixCodedTerms, TestRollingBuffer, TestPhrasePrefixQuery, Test4GBStoredFields, TestCustomSearcherSort, TestExplanations, TestIndexInput, TestMultiThreadTermVectors, TestTermInfosReaderIndex, TestSearchAfter, Test2BPositions, TestField, TestSimpleAttributeImpl, TestFlushByRamOrCountsPolicy, TestSimilarityBase, TestByteSlices, TestFlex, TestRecyclingByteBlockAllocator, TestCrash, Test2BTerms, TestSearcherManager, TestDeterminism, TestDemo, TestSpanExplanationsOfNonMatches, TestSubScorerFreqs, TestDocIdSet, TestFieldValueFilter, TestSpanMultiTermQueryWrapper, TestTermVectors, TestSurrogates, TestSimilarity2, Nested1, TestParallelReaderEmptyIndex, TestShardSearching, TestBlockPostingsFormat3, TestPagedBytes, TestCustomNorms, Before3, Before3, TestSegmentReader, TestMatchAllDocsQuery, TestTopDocsCollector, TestNoMergeScheduler, TestDeletionPolicy, Nested1, ThrowInUncaught, TestMultiTermConstantScore, TestPhraseQuery, TestGraphTokenizers, TestBitVector, TestPerFieldPostingsFormat2, TestSegmentTermEnum, TestVersion, TestNGramPhraseQuery, TestBackwardsCompatibility3x, TestRegexpRandom2, TestDirectoryReaderReopen, TestCompoundFile, TestBlockPostingsFormat, TestNRTThreads, TestPayloads, TestTransactions, TestTermRangeQuery, TestSmallFloat, TestFSTs, TestNorms, TestLookaheadTokenFilter, TestDuelingCodecs, TestAtomicUpdate, TestTermsEnum, TestMultiMMap, TestTimeLimitingCollector, TestTopDocsMerge, TestNRTManager, TestArrayUtil, TestBufferedIndexInput, TestIndexWriterForceMerge, TestIndexWriterCommit, TestWeakIdentityMap, TestTypePromotion, TestSimpleExplanations, TestStressIndexing, TestSnapshotDeletionPolicy, TestNRTReaderWithThreads, TestTieredMergePolicy, TestConsistentFieldNumbers, TestCrashCausesCorruptIndex, TestNumericUtils, TestMultiValuedNumericRangeQuery, TestCharTermAttributeImpl, TestRollingUpdates, TestPrefixInBooleanQuery, TestBytesRefHash, TestRamUsageEstimatorOnWildAnimals, TestFieldCacheRangeFilter, TestPayloadSpans, TestMixedCodecs, TestSegmentTermDocs, TestFieldCacheSanityChecker, TestDoc, TestMergeSchedulerExternal, TestOmitTf, TestDisjunctionMaxQuery, TestSimpleSearchEquivalence, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, TestPayloadNearQuery, TestFilteredQuery, TestPayloadExplanations, TestDocsAndPositions, TestTransactionRollback, TestCompiledAutomaton, TestSentinelIntSet, TestIndexableField, TestBooleanQuery, TestAutomatonQuery, TestComplexExplanationsOfNonMatches, TestRegexpQuery, TestDocCount, TestSearchForDuplicates, TestFilteredSearch, NestedSetupChain, NestedTeardownChain, Nested, Nested, TestDateFilter, TestSpansAdvanced, TestConstantScoreQuery, TestDateTools, TestSearch, TestReaderClosed, TestElevationComparator, TestAutomatonQueryUnicode, TestDateSort, TestBinaryDocument, TestSpanFirstQuery, TestPriorityQueue, TestDocBoost, TestMockCharFilter, TestIsCurrent, TestPrefixFilter, TestBitUtil, TestVersionComparator, TestCachingTokenFilter, TestTermdocPerf, TestTerm, TestLucene40PostingsFormat, TestAllFilesHaveCodecHeader, TestBagOfPositions]
DWPT assert tripped again  tripped the spooky DWPT ram used on flush assert in http://jenkins.sd-datasolutions.de/job/Lucene-Solr-4.x-Linux/2472/ It reproduces for me: Full failure:
******
core.src.java.org.apache.lucene.index.DocumentsWriterFlushControl.DocumentsWriterFlushControl.assertMemory(), false, comments
core.src.java.org.apache.lucene.index.DocumentsWriterFlushControl.DocumentsWriterFlushControl.assertNumDocsSinceStalled(boolean), false, new_method
core.src.java.org.apache.lucene.index.DocumentsWriterFlushControl.DocumentsWriterFlushControl.updateStallState(), false, test_method
#####
lucene-4.0
LUCENE-4479
https://issues.apache.org/jira/browse/LUCENE-4479
TokenSources.getTokenStream() doesn't return correctly for termvectors with positions but no offsets The javadocs for TokenSources.getTokenStream(Terms, boolean) state: "Low level api. Returns a token stream or null if no offset info available in index. This can be used to feed the highlighter with a pre-parsed token stream" However, if the Terms instance passed in has positions but no offsets stored, a TokenStream is incorrectly returned, rather than null. This has the effect of incorrectly highlighting fields with term vectors and positions, but no offsets. All highlighting markup is prepended to the beginning of the field.
doesn't return correctly for termvectors with positions but no offsets The javadocs for  state: "Low level api. Returns a token stream or null if no offset info available in index. This can be used to feed the highlighter with a pre-parsed token stream" However, if the Terms instance passed in has positions but no offsets stored, a is incorrectly returned, rather than null. This has the effect of incorrectly highlighting fields with term vectors and positions, but no offsets. All highlighting markup is prepended to the beginning of the field.
******
highlighter.src.java.org.apache.lucene.search.highlight.TokenSources.TokenSources.hasPositions(Terms), true
highlighter.src.java.org.apache.lucene.search.highlight.TokenSources.TokenSources.TokenStream(Terms, boolean), true
highlighter.src.java.org.apache.lucene.search.highlight.TokenSources.TokenSources.getTokenStream(IndexReader, int, String), true
core.src.test.org.apache.solr.highlight.HighlighterTest.HighlighterTest.testTermVectorWithoutOffsetsHighlight(), false, new_method
core.src.java.org.apache.solr.highlight.DefaultSolrHighlighter.DefaultSolrHighlighter.doHighlightingByHighlighter(Query, SolrQueryRequest, NamedList, int, StoredDocument, String), true
highlighter.src.test.org.apache.lucene.search.highlight.TokenSourcesTest.TokenSourcesTest.testTermVectorWithoutOffsetsThrowsException(), false, new_method
#####
lucene-4.0
LUCENE-4461
https://issues.apache.org/jira/browse/LUCENE-4461
Multiple FacetRequest with the same path creates inconsistent results Multiple FacetRequest are getting merged into one creating wrong results in this case: FacetSearchParams facetSearchParams = new FacetSearchParams(); facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath("author"), 10)); facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath("author"), 10)); Problem can be fixed by defining hashcode and equals in certain way that Lucene recognize we are talking about different requests. Attached test case.
Multiple with the same path creates inconsistent results with the same path creates inconsistent results getting merged into one creating wrong results in this case: Problem can be fixed by defining hashcode and equals in certain way that Lucene recognize we are talking about different requests.
******
facet.src.test.org.apache.lucene.facet.TestSameRequestAcculumation.TestSameRequestAcculumation.setUp(), false, new_method
facet.src.test.org.apache.lucene.facet.TestSameRequestAcculumation.TestSameRequestAcculumation.tearDown(), false, new_method
facet.src.test.org.apache.lucene.facet.TestSameRequestAcculumation.TestSameRequestAcculumation.testTwoSameRequests(), false, new_method
facet.src.java.org.apache.lucene.facet.search.StandardFacetsAccumulator.StandardFacetsAccumulator.accumulate(ScoredDocIDs), true
facet.src.java.org.apache.lucene.facet.search.StandardFacetsAccumulator.StandardFacetsAccumulator.initArraysByTotalCounts(FacetArrays, int, int), true
#####
lucene-4.0
LUCENE-4511
https://issues.apache.org/jira/browse/LUCENE-4511
TermsFilter might return wrong results if a field is not indexed or not present in the index TermsFilter returns if a term returns null from AIR#terms(term) while it should just continue. I will upload a test & fix shortly
might return wrong results if a field is not indexed or not present in the index   returns if a term returns null from while it should just continue. I will upload a test & fix shortly
******
spatial.src.java.org.apache.lucene.spatial.prefix.TermQueryPrefixTreeStrategy.TermQueryPrefixTreeStrategy.makeFilter(SpatialArgs), true
queries.src.test.org.apache.lucene.queries.BooleanFilterTest.BooleanFilterTest.getTermsFilter(String, String), false, test_method
queryparser.src.java.org.apache.lucene.queryparser.xml.builders.TermsFilterBuilder.TermsFilterBuilder.getFilter(Element), true
queries.src.java.org.apache.lucene.queries.TermsFilter.TermsFilter.addTerm(Term), true
queries.src.java.org.apache.lucene.queries.TermsFilter.TermsFilter.getDocIdSet(AtomicReaderContext, Bits), true
queries.src.java.org.apache.lucene.queries.TermsFilter.TermsFilter.equals(Object), true
queries.src.java.org.apache.lucene.queries.TermsFilter.TermsFilter.hashCode(), true
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.testCachability(), false, test_method
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.testMissingTerms(), false, test_method
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.testMissingField(), false, new_method
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.testFieldNotPresent(), false, new_method
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.testSkipField(), false, new_method
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.testRandom(), false, new_method
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.termsFilter(boolean, Term...), false, new_method
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.termsFilter(boolean, Collection<Term>), false, new_method
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.testHashCodeAndEquals(), false, new_method
queries.src.test.org.apache.lucene.queries.TermsFilterTest.TermsFilterTest.testNoTerms(), false, new_method
#####
lucene-4.0
LUCENE-4568
https://issues.apache.org/jira/browse/LUCENE-4568
Integer overflow in PagedBytes.PagedBytesData{In,Out}put.getPosition PagedBytes.PagedBytesData {In,Out} put.getPosition overflow when there are more than Integer.MAX_VALUE bytes.
Integer overflow overflow when there are more than bytes
******
core.src.test.org.apache.lucene.util.TestPagedBytes.TestPagedBytes.testOverflow(), false, new_method
core.src.java.org.apache.lucene.util.PagedBytes.PagedBytes.PagedBytes(int), true
core.src.java.org.apache.lucene.util.PagedBytes.PagedBytes.getPosition(), true
#####
lucene-4.0
LUCENE-4585
https://issues.apache.org/jira/browse/LUCENE-4585
Spatial RecursivePrefixTreeFilter has some bugs with indexing non-point shapes RecursivePrefixTreeFilter has some bugs that can occur when searching indexed shapes. One bug is an unpositioned termsEnum. It through an exception in testing; I'm not sure what its effects would be in production. The other couple bugs are hard to describe here but were rare to occur in extensive testing. The effects were probably a slim chance of matching an indexed shape near the query shape. And SpatialPrefixTree does not support an indexed shape that covers the entire globe. These bugs were discovered during development of tests for RPTF LUCENE-4419 which I will submit shortly.
Spatial has some bugs with indexing non-point shapes has some bugs that can occur when searching indexed shapes. One bug is an unpositioned. It through an exception in testing; I'm not sure what its effects would be in production. The other couple bugs are hard to describe here but were rare to occur in extensive testing. The effects were probably a slim chance of matching an indexed shape near the query shape. And does not support an indexed shape that covers the entire globe. These bugs were discovered during development of tests for RPTF LUCENE-4419 which I will submit shortly.
******
spatial.src.java.org.apache.lucene.spatial.prefix.tree.QuadPrefixTree.QuadPrefixTree.QuadCell(byte[], int, int), true
spatial.src.java.org.apache.lucene.spatial.prefix.tree.QuadPrefixTree.QuadPrefixTree.QuadCell(String), true
spatial.src.java.org.apache.lucene.spatial.prefix.tree.QuadPrefixTree.QuadPrefixTree.QuadCell(String, SpatialRelation), true
spatial.src.java.org.apache.lucene.spatial.prefix.PointPrefixTreeFieldCacheProvider.PointPrefixTreeFieldCacheProvider.readShape(BytesRef), true
spatial.src.java.org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree.SpatialPrefixTree.recursiveGetNodes(Node, Shape, int, boolean, Collection<Node>), true
spatial.src.java.org.apache.lucene.spatial.prefix.RecursivePrefixTreeFilter.RecursivePrefixTreeFilter.getDocIdSet(AtomicReaderContext, Bits), true
spatial.src.java.org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree.GeohashPrefixTree.GhCell(String), true
spatial.src.java.org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree.GeohashPrefixTree.GhCell(byte[], int, int), true
spatial.src.java.org.apache.lucene.spatial.prefix.tree.Node.Node.fixLeaf(), true
spatial.src.java.org.apache.lucene.spatial.prefix.tree.Node.Node.getSubCells(Shape), false, comments
spatial.src.java.org.apache.lucene.spatial.prefix.tree.Node.Node.isLeaf(), false, comments
spatial.src.java.org.apache.lucene.spatial.prefix.tree.Node.Node.Node(SpatialPrefixTree, byte[], int, int), true
spatial.src.java.org.apache.lucene.spatial.prefix.tree.Node.Node.Node(SpatialPrefixTree, String), true
#####
lucene-4.0
LUCENE-4633
https://issues.apache.org/jira/browse/LUCENE-4633
DirectoryTaxonomyWriter.replaceTaxonomy should refresh the reader While migrating code to Lucene 4.0 I tripped it. If you call replaceTaxonomy() with e.g. a taxonomy index that contains category "a", and then you try to add category "a" to the new taxonomy, it receives a new ordinal! The reason is that replaceTaxo doesn't refresh the internal IndexReader, but does clear the cache (as it should). This causes the next addCategory to not find category "a" in the cache, and not in the reader instance at hand. Simple fix, I'll attach a patch with it and a test exposing the bug.
should refresh the reader While migrating code to Lucene 4.0 I tripped it. If you call with e.g. a taxonomy index that contains category "a", and then you try to add category "a" to the new taxonomy, it receives a new ordinal! The reason is that replaceTaxo doesn't refresh the internal , but does clear the cache (as it should). This causes the next to not find category "a" in the cache, and not in the reader instance at hand. Simple fix, I'll attach a patch with it and a test exposing the bug.
******
facet.src.java.org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.DirectoryTaxonomyWriter.replaceTaxonomy(Directory), true
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestDirectoryTaxonomyWriter.TestDirectoryTaxonomyWriter.testReplaceTaxonomy(), false, test_method
#####
lucene-4.0
LUCENE-4469
https://issues.apache.org/jira/browse/LUCENE-4469
many Test2B* tests appear to be useless for some reason (I guess debugging) these explicitly disable checkindex on MDW and call checkindex themselves. but they dont check its return value! So these tests could currently all be failing and we wouldnt know (since If i remember right, checkindex catches throwable and records it inside its status) These tests should be changed to just let MDW run the checkindex, or use _TestUtil.checkIndex. We should also search the codebase for any other tests doing this.
many tests appear to be useless for some reason (I guess debugging) these explicitly disable on MDW and call themselves. but they dont check its return value! So these tests could currently all be failing and we wouldnt know (since If i remember right, catches throwable and records it inside its status) These tests should be changed to just let MDW run the We should also search the for any other tests doing this.
******
codecs.src.test.org.apache.lucene.codecs.pulsing.TestPulsingReuse.TestPulsingReuse.testNestedPulsing(), false, test_method
core.src.test.org.apache.lucene.index.Test2BPostings.Test2BPostings.test(), false, test_method
#####
lucene-4.0
LUCENE-4182
https://issues.apache.org/jira/browse/LUCENE-4182
DocumentsWriterFlushControl.assertMemory tripped by NGramTokenizerTest.testRandomStrings  Build: http://jenkins.sd-datasolutions.de/job/Lucene-Solr-trunk-Linux-Java6-64/1090/ 1 tests failed. REGRESSION: org.apache.lucene.analysis.ngram.NGramTokenizerTest.testRandomStrings Error Message: some thread(s) failed Stack Trace: java.lang.RuntimeException: some thread(s) failed at __randomizedtesting.SeedInfo.seed([256D0DE54BD0473A:ADE40D5BE8D4100F]:0) at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:463) at org.apache.lucene.analysis.ngram.NGramTokenizerTest.testRandomStrings(NGramTokenizerTest.java:106) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1969) at com.carrotsearch.randomizedtesting.RandomizedRunner.access$1100(RandomizedRunner.java:132) at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:814) at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:875) at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:889) at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50) at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:32) at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45) at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55) at org.apache.lucene.util.TestRuleReportUncaughtExceptions$1.evaluate(TestRuleReportUncaughtExceptions.java:68) at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48) at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48) at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:821) at com.carrotsearch.randomizedtesting.RandomizedRunner.access$700(RandomizedRunner.java:132) at com.carrotsearch.randomizedtesting.RandomizedRunner$3$1.run(RandomizedRunner.java:669) at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:695) at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:734) at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:745) at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45) at org.apache.lucene.util.TestRuleReportUncaughtExceptions$1.evaluate(TestRuleReportUncaughtExceptions.java:68) at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38) at org.apache.lucene.util.TestRuleIcuHack$1.evaluate(TestRuleIcuHack.java:51) at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55) at org.apache.lucene.util.TestRuleNoInstanceHooksOverrides$1.evaluate(TestRuleNoInstanceHooksOverrides.java:53) at org.apache.lucene.util.TestRuleNoStaticHooksShadowing$1.evaluate(TestRuleNoStaticHooksShadowing.java:52) at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:36) at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48) at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:56) at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:605) at com.carrotsearch.randomizedtesting.RandomizedRunner.access$400(RandomizedRunner.java:132) at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:551) Build Log: [...truncated 3668 lines...] [junit4] Suite: org.apache.lucene.analysis.ngram.NGramTokenizerTest [junit4] ERROR 3162s J1 | NGramTokenizerTest.testRandomStrings [junit4] > Throwable #1: java.lang.RuntimeException: some thread(s) failed [junit4] > at __randomizedtesting.SeedInfo.seed([256D0DE54BD0473A:ADE40D5BE8D4100F]:0) [junit4] > at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:463) [junit4] > at org.apache.lucene.analysis.ngram.NGramTokenizerTest.testRandomStrings(NGramTokenizerTest.java:106) [junit4] > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) [junit4] > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) [junit4] > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) [junit4] > at java.lang.reflect.Method.invoke(Method.java:597) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1969) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.access$1100(RandomizedRunner.java:132) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:814) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:875) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:889) [junit4] > at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50) [junit4] > at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:32) [junit4] > at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45) [junit4] > at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55) [junit4] > at org.apache.lucene.util.TestRuleReportUncaughtExceptions$1.evaluate(TestRuleReportUncaughtExceptions.java:68) [junit4] > at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48) [junit4] > at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:821) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.access$700(RandomizedRunner.java:132) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$3$1.run(RandomizedRunner.java:669) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:695) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:734) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:745) [junit4] > at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45) [junit4] > at org.apache.lucene.util.TestRuleReportUncaughtExceptions$1.evaluate(TestRuleReportUncaughtExceptions.java:68) [junit4] > at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38) [junit4] > at org.apache.lucene.util.TestRuleIcuHack$1.evaluate(TestRuleIcuHack.java:51) [junit4] > at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55) [junit4] > at org.apache.lucene.util.TestRuleNoInstanceHooksOverrides$1.evaluate(TestRuleNoInstanceHooksOverrides.java:53) [junit4] > at org.apache.lucene.util.TestRuleNoStaticHooksShadowing$1.evaluate(TestRuleNoStaticHooksShadowing.java:52) [junit4] > at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:36) [junit4] > at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48) [junit4] > at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:56) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:605) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.access$400(RandomizedRunner.java:132) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:551) [junit4] > Throwable #2: java.lang.RuntimeException: Thread threw an uncaught exception, thread: Thread[Thread-548,5,] [junit4] > at com.carrotsearch.randomizedtesting.RunnerThreadGroup.processUncaught(RunnerThreadGroup.java:96) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:857) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.access$700(RandomizedRunner.java:132) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$3$1.run(RandomizedRunner.java:669) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:695) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:734) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:745) [junit4] > at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45) [junit4] > at org.apache.lucene.util.TestRuleReportUncaughtExceptions$1.evaluate(TestRuleReportUncaughtExceptions.java:68) [junit4] > at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38) [junit4] > at org.apache.lucene.util.TestRuleIcuHack$1.evaluate(TestRuleIcuHack.java:51) [junit4] > at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55) [junit4] > at org.apache.lucene.util.TestRuleNoInstanceHooksOverrides$1.evaluate(TestRuleNoInstanceHooksOverrides.java:53) [junit4] > at org.apache.lucene.util.TestRuleNoStaticHooksShadowing$1.evaluate(TestRuleNoStaticHooksShadowing.java:52) [junit4] > at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:36) [junit4] > at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48) [junit4] > at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:56) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:605) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner.access$400(RandomizedRunner.java:132) [junit4] > at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:551) [junit4] > Caused by: java.lang.AssertionError: ram was 41605728 expected: 40986328 flush mem: 27371536 activeMem: 14234192 pendingMem: 0 flushingMem: 2 blockedMem: 0 peakDeltaMem: 3715948 [junit4] > at __randomizedtesting.SeedInfo.seed([256D0DE54BD0473A]:0) [junit4] > at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:114) [junit4] > at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:181) [junit4] > at org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:348) [junit4] > at org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1174) [junit4] > at org.apache.lucene.index.IndexWriter.addDocuments(IndexWriter.java:1134) [junit4] > at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:157) [junit4] > at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:145) [junit4] > at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:555) [junit4] > at org.apache.lucene.analysis.BaseTokenStreamTestCase.access$000(BaseTokenStreamTestCase.java:57) [junit4] > at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:414) [junit4] > [junit4] 2> TEST FAIL: useCharFilter=false text='s z zxpw qu j jlpc \u10bd\u10bb\u10b4\u10c3\u10ab\u10a5 ag zuv vtjj \u0492\ue462\ue1b8\u02cd\uaf99\u026a cxv xcyqyq m\ud9cbb \u1b84\u1ba7\u1b8c\u1b86 zq \u2059 fkg \u16f1\u16cf\u16f5\u16ca\u16ce\u16e4\u16a2 ya[ik \u0009^1 ;&# \uaa44\uaa13\uaa5b\uaa59\uaa12\uaa16 \ua965\ua97d\ua964\ua97d\ua968\ua974\ua966 \ue5db\uac21 \ue714\u6564et\u885d kjkqd .{ \ue932 \uf987 smyv bynhphqp kbv ] ]{1 qtrr \u2de6\u2df9\u2dfd \u042f$ twbpfnaz \uf12a\u8760c\uf242\u65dd ptwfoph [{1, t xmbvs OmFr \ua9d8\ua9c3\ua9a0\ua9c8\ua9db\ua9da\ua9c8\ua983 \u10431 \ud7c9\ud7d9\ud7e1\ud7fe\ud7d6\ud7b6 \u5649\uabdd~ e iceex hlglb ppcbklfa spq \ua778\ua7ae \u258d\u2594\u2583\u259b\u2584\u258d\u2592\u259e \u034f jhbacf \u1ba7\u1b92\u1ba2\u1b88 yi q \ueab0\u9dc33\u065b\u0134\u0793 (h{0,5 cgg Swe \u0398\uf2c5[\u1920\ue5f9\u0589\u813a o \ua5ea\ua52f r bWf \u319c\u3198\u319d\u3193\u319e\u319a z|z.) \u24e3\u24de\u24d4\u2487\u24ce vqaaafd ? \u2bb9\u2b65\u2b3b\u2b9e -yv.{0,5} \ue611\u01033\u0342\uea5c\u4164)\u0677 jnfaymol j \u2508 vvge \uf437\uc22d pfoubst av \u26a3\u26b8\u266c\u2695\u269b \u4ddf \ua15d\ua3ef\ua2c2 </p focqf \u014d\ub45ei -h \ua8fe\ua8f1\ua8ec\ua8fd\ua8f7 \u5ca7\u07cf ' \u02c0\u0342\ue884\u2890v\u9f78d oqsnpewn xbblilaf tndwtfqtj bjedbercm By \u23f2 \u2df7\u2dfd\u2de2\u2de4 vdxdw \u6809\u9e9cf\ud632\uf524 \u10816 \ua800\ua81f\ua81a\ua821\ua80a\ua808 \ua73d\ua781\ua7ae all > \ua820\ua80b \u30d2\u30df\u30ff\u30f3 \u1008b vjijpdh </script> \u10164\u10183 myzcm \ua726 vkp iljo xetk athqzre fx juik \u0692 f(r 5\u7485\u6675 lvle \u79300\u2370e\ufdab\uff94\uffb7\uf855T \u2618k ?{0,5}jn[?eh( qpawziia ctb \u2b33\u2bad\u2bb6 uq \ufe3c\u0794\uf225 \ua966\ue47a\u058a\ue094 hmb vef gunpnry \u0175 {0 lgr rspeigro \u0cf9\u0cfe dqg \u1b40\u1b03\u1b67\u1b08\u1b00\u1b22\u1b0f acvfnq \u001e u \uf3c5\u29ff\uf172\u05e7\uf12f\u0651 fhb m lxjaa \u1c4d\u1c07\u1c3f\u1c09\u1c1b gdg \u1006\u105c\u1080\u1057\u1017\u1044\u1090 9\u043c\u523d5h\u0350 \u1d36a\u1d373 hxvvq hp ;--> 9\u4362f\ue6cab\\M\u0213 \u1c40\u1c2a\u1c40\u1c02 \u118a#\ueedb\uf9946 trm \ue434dP\uffc9\ua0fe0\u001c \u24e0\u2468\u24c9\u24ba\u24e4\u2467 cylyAdr \u0455\u038e\uf37ak ggk hodch \u10850\u10850\u1085c\u10851\u10851 \u1103 vmbr \ub7963 \u0d5e\u0d6b\u0d39\u0d59\u0d1d\u0d0c\u0d3c \u015f\u01f0H\ucd55\u3fe0\u000e \ufe16\ufe1a\ufe18\ufe1e\ufe12\ufe11\ufe1e\ufe11\ufe1f )(]).{0,5}q \u0382\ud0047F\u5267\ue3c2 </ &# Tl\u0013\u6363\u11c61 gqftlg S <!-- vy \u1fab vbe uth uep \u26a00\u25de5 \u07b2\ua5a2 pojdwa _\uad8b1b\u8d40 dzjra patxxtyk \u17ed\uff16o kvqyelu \u31e6\u31ce\u31eb\u31e8 \ubcc2\u037ezH zx \u1e82\u1eca\u1ea9 zaki \u050c\u1b6e\uef15> \u2c60\u2c75\u2c78\u2c70\u2c65 x \u1fff\u1f27\u1fee \n\\\"\\' \uf0c7v\u3347Xy\uf83b \uebacd\u7a90f\ub5a7 rowfh \u1fe11\ufa3f\u0016\u96a6 pja euu \u8edc\ue07b\ubdf4\ub689\ub8b31h\u5a38 acqheoidhmu 7\u24f0\u087f\u8578\u4555 ksszgdl \u12d7\u12d8\u133b\u1211\u1280 kyfcnb ejnep \"p \u1968\u1967\u1950\u1956 ejinsu ohkiz rcksdpve \ufcf7\u016a \ua5bd\ua504\ua5ea\ua540\ua5d5\ua627 \ue6a7\u7764\u7d60d\ub4cc\u5156\ufb49\u06b3\u0001 fervu xn \u2d40\u2d5d\u2d3a\u2d7f\u2d4e vgs fsut {0,5}s \u10060\u10072\u10075 xxgks \u2813\u28a1 wusp ejjafxhc ihde zitwdlxavrh eh poh ri x\u667c6\u01f6 paie \ua8e6\ua8ef\ua8e0\ua8f5\ua8f2\ua8fa \u0685 pmacb jeo \uff52\uff77 zy '<p> ?rel \\' \u27d6\u27ce tu \ufe17\ufe13 zb shlkou K zvjowv gy sl \uaac8\uaad1\uaab4\uaaac\uaa97\uaad7\uaab5\uaaa8\uaac7\uaad8\uaa9e 4 \u0003' \\\"\" \u20e6\u20e3\u20f5 nfkwsd rwlw \uf41f ewv az \u07bb qtc ndUYsp \u3160\u314e\u3136\u3139\u3185\u3133\u3175 debq vdfd \u7d87f\u1983\ue1d4.- yeo \u0485ml z <?\n'\\'</s < u fhk \u07f2\u0325\u16a242 ptma \u0a85\u0ad8\u0a82\u0a90\u0ae0 \u6f84\u8a56\u1dec\u450e\u2d22c\u10da6f \u1d7c\u1d2f \ufa3c\u1430\u2b9be\u0770\u0018 lcgnclfdii ,\u54cf\u01ad\ubcfdf \u103cb\u103d9 \" | \u00f0\u00c6\u00bb\u00cb\u00cc\u00dd\u00fb rmi \ufc97\ufd78\ufbce ynclv \ufcca\ufc0b\ufb6a\ufc25 hpnukxtjn ptuoqdda \u00e2\u597a\u0010\u2883\ub3cd\ue10f\uf3c8 j muwt bw \u37c3\ua4873 x \u87fe\u4a5f2\ufda5 \u2ad79\u2b5bf\u2b21c\u2ad13\u2b102 skrl rgepp \u1939\u1920\u1948\u1904\u194c \u07be \ueaef\uedfe\u0481\u010d \ubb01a\uce0a\u0668 </ ancxfe fhgaca nk 3\u45aa s]o]{0, \u7152 tv \u044e\u02d6\u09e8 \u0a66\u0a69\u0a2e\u0a47\u0a62\u0a1c\u0a0b \u173f\u172b \u0504\u0515\u0504\u0520 \u9d67\uc6e0\ue13e\r\u6a67z \u02b3V\u6ff7\uf534\ueade\ue092\u06a2 b ebchlicd \u04f9\u4141d\u0011\ue6ae \u0937\u097b\u092c\u096f\u0928\u0944\u0941 iwwrdqu nkkz mu[[. \\'-->& ckbhwuat \u2e01\u2e2e\u2e12\u2e33\u2e30 >c \u1118\u1138\u1172\u1137\u11e8\u110a\u1147 \u0467 o \ue87ff \u1090d\u10900 nreqyjvatc \ued5a\uf44b udmc wnmg \u20da\u20d7\u20d4\u20ea\u20e4 \u2e2d\u2e33\u2e34 \u2ff9\u2ff1\u2ffd\u2ff3 \u1aa7\u1a28 wmwm \u2554\u2571\u250f\u2558\u2521\u250d\u252b o kjes uq \ufe1f\ufe13\ufe16\ufe14\ufe15\ufe13\ufe17\ufe1d aeebxvf o[hk][( veh \u06e2\u00ad\u0125\ued9c\u2223 rwf lcmtqev Ksvu \ue01c4\ue0146\ue0182 jtx ljpqre etzwnjnn BOmuXt bs \u0cac\u0cb7 lij eu x \ufe2b\u1054\u0454\ue410 rd fscl &#x& gkmrw \u2cd4\u2cfd\u2cf3\u2c88\u2cf1\u2c98 \u0c29\u0c09\u0c75\u0c4e zrhpfAdpax \u05477 &Ps \ua6dc\ua6ae \u5b38e\ue7dbZ\ud038e\ua712b b{1,5})h rt \u185b \u31a9\u31ba\u31a9\u31bb\u31b7\u31b2\u31a9\u31ba \u2cf5\u2cfa\u2ca9\u2cfd ovxf xegi \uaec48\u09bb\ufd01\ua107\uf5c2 \u2f70\u2f9a cppxm zzi ?><! epsrs \u076e\u0761\u0754\u075e\u075f\u075a\u0754\u0763 \u1093f\u10922 ` ]{0, & qgbiwn < vyge acmvidw xbwgrppk \uf612p\u05f5^\u048f\u056d \ud6436\u9cde0\u274e\u0592 tamcca \ufd7b\ufbf9\ufb88\ufbae tusifiwj \u000f\u0600\uef93}\u28e7\u599ab zk \u09ed\u09d1 > y \u013d\u0171\u011e\u0142\u012f irxvlpgbl xiw tjdelwn lxlojUEj Ttik xtoxop dansms \u29b5\u2983\u299a\u2988\u29b1 |(](|s[ tgllq vmuy ksizv \ua77f\ua74a\ua759\ua799\ua7f4\ua75b\ua7ee\ua770 vvm agcuehf syu f \ufe79\u001d raxj \u8f2aH \u207a\u2086\u207f\u208b\u207b\u2088\u2098 \ud813b\u1f70 vpawp hevp \u4436\ueace9\u2133 koaba mikat \u31fe\u31f8\u31f6\u31f2\u31f5\u31f3\u31f2\u31fd\u31ff \ue19b\u0424\u04f6K\ucab95 thmvy \uffa6\uff6e\uff1c\uff98\uff82\uff44 \\'\\ pahfinz \uf650\ue338\u15b4\u0011\" \uf9a3\ufac3\uf905\uf9d1\ufa80\uf976 ' \u1357\u1244\u1336\u1209\u124f\u1338 o pora \u3107\u3106\u312e\u310e\u3126\u3115\u3108 \u0014 \ua92c\ua905\ua920 \u8dfe2\u31102\ue695\uf742 nxsxezzn quende \u2d6e\u2d72\u2d4a\u2d6f aqroaomb w.b({0 wtx ot \u2116\u2120\u2131 \uff65\u25ed\u023e\u3389\uecca\u89590\ud4325\ua3e9dx vj \uec30\u06057 oiq sfncjcxm jkcycin \u040e anrwz \u024c\u42e01 <!--# fe qnf lq 6;T awm huxr d)l \ue404\ucfb1 \ua557\ua5be\ua588\ua5f3\ua58a\ua57c\ua5a6 L0\uf8ed q pbyvjuqtq \u0c51\u0c50 hsjfsnig glgw \u4d2d\u0342T\ufb58D\u0ed5 fmft ?><! p \u3b83a\ue92b\uc2d0\u0c95\u0091 \u2565 \u0a8e\u0ab2\u0acc\u0ac6 \u10a06\u10a27 ke bromtk hoixcmuvf onmotohc m\uf434\u2451 pesvv glrugbidmi aRRrrF ofopzwucn \u32b8\u86f4\uf05b \u2c98\u2c82\u2c97 ?s|]ql \u13c3\u13e3\u13e2\u13ac mrgslmse \uf717 )wlax]|[ atq \u03ea}\u105b9a\u0443 ck \ufa368\u0e3a \u5cae4&\ua107\u8581\uf5d0 \uf2d03\u0005 <p>?><?-- \u2458\u2454\u2450\u2442\u2450\u2446\u2452\u2451\u2444 \u174f\u1741\u174f wksf gsunjwo \u10348\u10344 \u4a4e\u10e36a\uf305 ></s \u0dfa\u0dbb\u0df5\u0d81\u0dc1\u0db2\u0dd7 otfvyni vc x \u0816\u0833\u082b\u0822\u0829\u080e \u477f\u0652\u03c3\u0301 \ua1cac \u16c6\u001c \ufe9c\ufe92 zzTWohEr \u0e48\u0e25\u0e23\u0e1c\u0e6b tqff x.)mpp f \uf876W\uea505I\u72df\u8c470 kt| xkyjxh ujerwftr \u2e5a\u2e78\u2e19\u2e44\u2e4d\u2e55\u2e47 msllr 5\ueac9\ufc31o \u039d\u9a5b\uf61b\ue5d5\u07b0\ue641\ua924\u07e5 g \ua907\ua900\ua92e \ua0468\ue2fb\ue945\uceaf \u16ea\u16fd\u16ea\u16b3\u16e3\u16ee\u16b1\u16cb\u16e8\u16bf \u1941\u1908\u1911\u194d\u193d\u1936\u1935 \u20d9\u20d8\u20eb\u20ea\u20ea\u20fa\u20ed jdrslh \u2153\u2171\u2180\u2181\u2161\u2153\u2160 \u0921 \u001c\u032ak \u10485\u10487 rqxuf I\ufce8 \u703e\u964d\u80bb\u6647\u739c\u75b5\u5f5d yu guanyrd rwuue c wv \u1c0c\u1c39\u1c21\u1c38 giu \u592da\u0440\u82a2\ue661 aqxoj \uac3f\u0018\u001a\ue7a8 uaj qio \u1e78\u1ee1\u1e27\u1e33 \u0220\u0227\u0209\u0229\u023f zrt crjd yh yzn nkndmyewfqzi \ud54a\uc064\u00dd\ucb2d\u4d6b jmohh \u0b3d\u0b0b\u0b0c\u0b10\u0b46\u0b45 pzwi \\\"- ahbbaye tiazuvfnfh Q\u0179 \u2c68\u2c77\u2c7c kbpre aobgk uzsxgtxp &;>< n r \u68ca0\u00b1 kaqtji ] yq =^ \u10a78\u10a6c\u10a60 \ufe64\ufe6e\ufe53 hjMQtFba swv \u1f193 &#x54c {0,5} qxj qeat \ufe52\ufe66\ufe65 \u069a\ued9c\u037a\u00e7\u0657|\u0400 rjc yaeqtzpgsu \u72bdO\u0517\u4d4fa\uc910\u5b30 @\u02f5 \ua726 y\ua148\uf2b4 oaq \u2325 \ue153X p zdb \ud6bbN\u218e} \u1033f\u10347\u10341 d \u10016\u10071 \u309b \u0da2\u0da9 {1 \u9ed2\u8b89\u0a42 wpv ot g oedh L\ub94f9u;Q \u0539\uaa76\u9e9b\u31d4 qje neizli xzxgeh \u2413 \u033f\u001f &#x3 \u2e34\u2e28\u2e5d\u2e08 xpne \u1029d zhj \u01ed\uf1f2a\ufca5# ]|yd{0 lgfxbowak fyuv ymsh ervcv \u22fc\u22bb \ufeaf\ufee6\ufeeb\ufef1\ufe71 ogrkop \u319d \ud4b6\u614db \u2a08\u2a1c\u2a2a\u2ace\u2a86\u2a10\u2af9\u2a58\u2a43\u2a3f\u2abb\u2acc < \u008b\uf68de\u06e5 \u104a2\u10482 qxurjt \uf7db\uf352\ue29c \"<p \u10286\u10280 hasmw lmga mu \ufd0f\ubee28\u0648\u9aea\u1013b6\u8174\u0006\ue2aed ` w mciod aysqvom qnrtdqmqu nf nexjvlz \u2182\u2154\u2154\u2184\u216c \u05d1\uf045\ucd7e5\u2af3\u16683 pw dva \\\"< \uf151\u0016\u053fe Y\u0002\u7872 \ufe1a\ufe17\ufe14\ufe18 jidqkxf bbr \u3190\u3196\u319e\u3194\u319b\u3194\u3195\u3191 llizhg wnkzu jfcz \u05349\u0011 \u5050l\u0391\u06a0 \u5ef8\u03bd\ubc21 \u001e \u0013\ufcc7 \ua8bb\ua8da pqsuegc ihux zgxsqjzsckix olu iext ptfvv A\u59c7\u050c nf cnteyylh m \uf0e2\u97f9t\u31183 lrtv <p></ <?&s jhwuauv OfwV \uecbf^\u03e3\uc563\ued78\u584e\ue177 \u169f\u1691\u169a\u169c\u1681 n ahfejklim \u00b7-\ufea9 dbg bwdkew slxkv \uf723 fmntdq qjkfhl a fr[[ \u1041a\u1044b gu \u1d355 e |])cev \u1033b\u10333\u10332 Y \u3027\u300d\u3019 w \u0770\u076f\u0761\u0775\u0760\u076c\u0774 pvduh pstk mzh rg \ua8d6 gtex \u2335\u237b\u2344\u238e fyns uirr \u0fc5\u0fad\u0f51 jwfpos bc \u03df\u03c5\u0395\u03b0\u0384 \u13186\u1302f\u1340e\u133e0 \u313b\u3136 oo{ upa \u19c2\u198f\u1998\u199e nko \u1d36a wro dpu \ua84f\ua878\ua852 Y\ucd66 btpml hefxu u \uf4fc\u0708\u0669\u9520 kkbiqh weecmoy gej xAm hn < frge wuc &#1564800 zfi xon ozbtjdbnjqx \u04de\u0421\u047c\u04d1 iy \ufe35\ufe47\ufe3e tyrbgo czunk o\u05a1 <?<p oz igye \u24af\u24d0\u249c\u248e\u24cf\u24bf \ud7c3\uf4c0 xeqodif pyl(a ni \u12387\u122db mmibcrb pboxwmy k ghtr lwqolgdvc \ud7e9\ud7ec\ud7d3\ud7cb\ud7ff &# \uaac1\uaa98\uaaba\uaab0\uaaa1 \u0c7d\uaa2d\u7302\u6246f\uce3e3 ixmms \uff1f\uff80\uffa4 \ue2d2\u01ed t?](k bai \u214e\u214b\u2143 npmzph \ua4fc\ua4e2\ua4d4\ua4d3\ua4d4 \ua82f\ua81e\ua82f\ua809 o \u2175\u216c sibzotjcl zwaoxf bchxwxe w oes iz rmiq cm \uf1b5\u05b6\u74a80\u06a2\ufcbc \u458b jbwscrrdaor pbra \ua7db\ua729\ua756\ua7f6\ua762\ua763 \uff64\ua5121\ue447\u05c6 akf suk lgpjc lixjys wjrf' [junit4] 2> NOTE: reproduce with: ant test -Dtestcase=NGramTokenizerTest -Dtests.method=testRandomStrings -Dtests.seed=256D0DE54BD0473A -Dtests.multiplier=3 -Dtests.locale=es_AR -Dtests.timezone=Asia/Baku -Dargs="-Dfile.encoding=UTF-8" [junit4] 2> [junit4] > (@AfterClass output) [junit4] 2> NOTE: test params are: codec=Lucene40: {dummy=MockFixedIntBlock(blockSize=1329)}, sim=RandomSimilarityProvider(queryNorm=true,coord=true): {dummy=DFR I(n)3(800.0)}, locale=es_AR, timezone=Asia/Baku [junit4] 2> NOTE: Linux 2.6.32-41-server amd64/Sun Microsystems Inc. 1.6.0_32 (64-bit)/cpus=8,threads=1,free=222774808,total=419037184 [junit4] 2> NOTE: All tests run in this JVM: [TestWordlistLoader, TestTurkishLowerCaseFilter, TestGermanMinimalStemFilter, TestSpanishAnalyzer, TestCJKWidthFilter, HunspellStemmerTest, TestDutchStemmer, TokenTypeSinkTokenizerTest, TestCharFilter, TestStemmerOverrideFilter, TestSolrSynonymParser, DateRecognizerSinkTokenizerTest, ShingleFilterTest, TestItalianLightStemFilter, TestThaiAnalyzer, TestPortugueseMinimalStemFilter, TestCharArraySet, TestKeepWordFilter, TestSnowball, TestLengthFilter, TestLimitTokenCountAnalyzer, TestFrenchLightStemFilter, TestItalianAnalyzer, TestLatvianStemmer, TestNorwegianLightStemFilter, TestGermanNormalizationFilter, TestPathHierarchyTokenizer, TokenRangeSinkTokenizerTest, TestHungarianAnalyzer, TestCharArrayIterator, TestCzechAnalyzer, TestSynonymMapFilter, TestBulgarianStemmer, CommonGramsFilterTest, TestPortugueseStemFilter, WikipediaTokenizerTest, TestCharTokenizers, TestRussianLightStemFilter, TestTypeTokenFilter, TestGermanAnalyzer, TestFinnishLightStemFilter, TestHindiAnalyzer, TestMappingCharFilter, TestBugInSomething, TestPerFieldAnalzyerWrapper, NGramTokenizerTest] [junit4] 2> [junit4] Completed on J1 in 3162.49s, 8 tests, 1 error <<< FAILURES! [...truncated 23 lines...] BUILD FAILED /mnt/ssd/jenkins/workspace/Lucene-Solr-trunk-Linux-Java6-64/checkout/build.xml:29: The following error occurred while executing this line: /mnt/ssd/jenkins/workspace/Lucene-Solr-trunk-Linux-Java6-64/checkout/lucene/build.xml:456: The following error occurred while executing this line: /mnt/ssd/jenkins/workspace/Lucene-Solr-trunk-Linux-Java6-64/checkout/lucene/common-build.xml:1435: The following error occurred while executing this line: /mnt/ssd/jenkins/workspace/Lucene-Solr-trunk-Linux-Java6-64/checkout/lucene/analysis/build.xml:101: The following error occurred while executing this line: /mnt/ssd/jenkins/workspace/Lucene-Solr-trunk-Linux-Java6-64/checkout/lucene/analysis/build.xml:38: The following error occurred while executing this line: /mnt/ssd/jenkins/workspace/Lucene-Solr-trunk-Linux-Java6-64/checkout/lucene/module-build.xml:62: The following error occurred while executing this line: /mnt/ssd/jenkins/workspace/Lucene-Solr-trunk-Linux-Java6-64/checkout/lucene/common-build.xml:1037: The following error occurred while executing this line: /mnt/ssd/jenkins/workspace/Lucene-Solr-trunk-Linux-Java6-64/checkout/lucene/common-build.xml:760: There were test failures: 139 suites, 891 tests, 1 error, 1 ignored Total time: 60 minutes 57 seconds Build step 'Execute shell' marked build as failure Archiving artifacts Recording test results Email was triggered for: Failure Sending email for trigger: Failure
tripped by Build: http://jenkins.sd-datasolutions.de/job/Lucene-Solr-trunk-Linux-Java6-64/1090/ 1 tests failed. REGRESSION: Error Message: some thread(s) failed
******
core.src.java.org.apache.lucene.index.DocumentsWriterFlushControl.DocumentsWriterFlushControl.assertMemory(), true
#####
lucene-4.0
LUCENE-4459
https://issues.apache.org/jira/browse/LUCENE-4459
TestWeakIdentityMap.testConcurrentHashMap fails periodically in jenkins There is either a bug, a test bug, or a jvm bug. I dont care which one it is, but lets fix the intermittent fail or disable the test.
fails periodically in jenkins There is either a bug, a test bug, or a jvm bug. I dont care which one it is, but lets fix the intermittent fail or disable the test.
******
core.src.test.org.apache.lucene.util.TestWeakIdentityMap.TestWeakIdentityMap.testSimpleHashMap(), false, test_method
core.src.test.org.apache.lucene.util.TestWeakIdentityMap.TestWeakIdentityMap.testConcurrentHashMap(), false, test_method
core.src.java.org.apache.lucene.util.WeakIdentityMap.WeakIdentityMap.hasNext(), true
core.src.java.org.apache.lucene.util.WeakIdentityMap.WeakIdentityMap.next(), true
core.src.java.org.apache.lucene.util.WeakIdentityMap.WeakIdentityMap.setNext(), true
#####
lucene-4.0
LUCENE-4485
https://issues.apache.org/jira/browse/LUCENE-4485
CheckIndex's term stats should not include deleted docs I was looking at the CheckIndex output on and index that has deletions, eg: 4 of 30: name=_90 docCount=588408 codec=Lucene41 compound=false numFiles=14 size (MB)=265.318 diagnostics = {os=Linux, os.version=3.2.0-23-generic, mergeFactor=10, source=merge, lucene.version=5.0-SNAPSHOT, os.arch=amd64, mergeMaxNumSegments=-1, java.version=1.7.0_07, java.vendor=Oracle Corporation} has deletions [delGen=1] test: open reader.........OK [39351 deleted docs] test: fields..............OK [8 fields] test: field norms.........OK [2 fields] test: terms, freq, prox...OK [4910342 terms; 61319238 terms/docs pairs; 65597188 tokens] test (ignoring deletes): terms, freq, prox...OK [4910342 terms; 61319238 terms/docs pairs; 70293065 tokens] test: stored fields.......OK [1647171 total field count; avg 3 fields per doc] test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc] test: docvalues...........OK [0 total doc count; 1 docvalues fields] If you compare the test: terms, freq, prox (includes deletions) and the next line (doesn't include deletions), it's confusing because only the 3rd number (tokens) reflects deletions. I think the first two numbers should also reflect deletions? This way an app could get a sense of how much "deadweight" is in the index due to un-reclaimed deletions...
term stats should not include deleted docs I was looking at the output on and index that has deletions If you compare the test: terms, freq, prox (includes deletions) and the next line (doesn't include deletions), it's confusing because only the 3rd number (tokens) reflects deletions. I think the first two numbers should also reflect deletions? This way an app could get a sense of how much "deadweight" is in the index due to un-reclaimed deletions...
******
core.src.java.org.apache.lucene.index.CheckIndex.CheckIndex.checkFields(Fields, Bits, int, FieldInfos, boolean, boolean), true
core.src.test.org.apache.lucene.index.TestCheckIndex.TestCheckIndex.testDeletedDocs(), false, test_method
#####
lucene-4.0
LUCENE-4653
https://issues.apache.org/jira/browse/LUCENE-4653
Make TestIndexWriter.testThreadInterruptDeadlock meaner Just tweaking the test to also call w.updateDocument (so we sometimes apply deletes) (Rob's idea) causes all sorts of fun failures ...
(so we sometimes apply deletes) (Rob's idea) causes all sorts of fun failures ... Just tweaking the test to also call Make meaner
******
codecs.src.java.org.apache.lucene.codecs.simpletext.SimpleTextFieldsReader.SimpleTextFieldsReader.SimpleTextFieldsReader(SegmentReadState), true
core.src.java.org.apache.lucene.index.ReadersAndLiveDocs.ReadersAndLiveDocs.dropReaders(), true
core.src.java.org.apache.lucene.index.ReadersAndLiveDocs.ReadersAndLiveDocs.writeLiveDocs(Directory), true
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.IndexerThreadInterrupt(), false, test_method
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.run(), true
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.testNoDocsIndex(), false, test_method
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.testThreadInterruptDeadlock(), false, test_method
core.src.java.org.apache.lucene.index.SegmentInfoPerCommit.SegmentInfoPerCommit.SegmentInfoPerCommit(SegmentInfo, int, long), true
core.src.java.org.apache.lucene.index.SegmentInfoPerCommit.SegmentInfoPerCommit.advanceDelGen(), true
core.src.java.org.apache.lucene.index.SegmentInfoPerCommit.SegmentInfoPerCommit.advanceNextWriteDelGen(), false, new_method
core.src.java.org.apache.lucene.index.SegmentInfoPerCommit.SegmentInfoPerCommit.getNextDelGen(), true
core.src.java.org.apache.lucene.index.SegmentInfoPerCommit.SegmentInfoPerCommit.clone(), true
core.src.java.org.apache.lucene.index.IndexWriter.IndexWriter.getReader(boolean), true
core.src.java.org.apache.lucene.codecs.lucene40.BitVector.BitVector.write(Directory, String, IOContext), true
#####
lucene-4.0
LUCENE-4480
https://issues.apache.org/jira/browse/LUCENE-4480
NoSuchElementException from AnalyzingSuggester Spinoff from LUCENE-3846 where Simon hit this ... it's a bug in FST's Util.TopNSearcher where it fails to handle queue exhaustion properly. I think WFSTSuggester isn't affected because by the time the queue is exhausted, the search is done (ie it never adds to the queue after it becomes exhausted).
isn't affected because by the time the queue is exhausted, the search is done (ie it never adds to the queue after it becomes exhausted). Spinoff from LUCENE-3846 where Simon hit this ... it's a bug in where it fails to handle queue exhaustion properly. I think
******
core.src.java.org.apache.lucene.util.fst.Util.Util.search(), true
core.src.java.org.apache.lucene.util.fst.Util.Util.addIfCompetitive(FSTPath<T>), true
suggest.src.test.org.apache.lucene.search.suggest.analyzing.AnalyzingSuggesterTest.AnalyzingSuggesterTest.testQueueExhaustion(), false, new_method
suggest.src.java.org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester.AnalyzingSuggester.lookup(CharSequence, boolean, int), true
#####
lucene-4.0
LUCENE-4671
https://issues.apache.org/jira/browse/LUCENE-4671
CharsRef.subSequence broken Looks like CharsRef.subSequence() is currently broken It is implemented as: @Override public CharSequence subSequence(int start, int end) { // NOTE: must do a real check here to meet the specs of CharSequence if (start < 0 || end > length || start > end) { throw new IndexOutOfBoundsException(); } return new CharsRef(chars, offset + start, offset + end); } Since CharsRef constructor is (char[] chars, int offset, int length), Should Be: @Override public CharSequence subSequence(int start, int end) { // NOTE: must do a real check here to meet the specs of CharSequence if (start < 0 || end > length || start > end) { throw new IndexOutOfBoundsException(); } return new CharsRef(chars, offset + start, end - start); }
broken is currently broken It is implemented as: Looks like Since
******
core.src.test.org.apache.lucene.util.TestCharsRef.TestCharsRef.testCharSequenceSubSequence(), false, test_method
core.src.test.org.apache.lucene.util.TestCharsRef.TestCharsRef.doTestSequence(CharSequence), false, refactoring
core.src.java.org.apache.lucene.util.CharsRef.CharsRef.subSequence(int, int), true
#####
lucene-4.0
LUCENE-4594
https://issues.apache.org/jira/browse/LUCENE-4594
Spatial PrefixTreeStrategy shouldn't index center-points with shapes together The Spatial PrefixTreeStrategy will index the center-point of a non-point shape it is given to index, in addition to the shape itself of course. The rationale was that this point could be picked up by PointPrefixTreeFieldCacheProvider for distance/sorting. However this approach is buggy since the distinction of grid cells between the center point and the shape itself is lost when the shape gets indexed down to max-levels precision - each grid cell therein appears to be another point that needs to be brought into memory. It's also possible that the shape is a LineString or some other non-trivial shape in which its center point isn't actually in the shape. Even if you knew this problem would never happen, I think you're better off indexing center points into another spatial field if you want them. Perhaps arguably this strategy could do that internally? Wether or not that ends up happening, I just want to remove the problematic behavior now.
for distance/sorting. However this approach is buggy since the distinction of grid cells between the center point and the shape itself is lost when the shape gets indexed down to max-levels precision - each grid cell therein appears to be another point that needs to be brought into memory. It's also possible that the shape is a or some other non-trivial shape in which its center point isn't actually in the shape. Even if you knew this problem would never happen, I think you're better off indexing center points into another spatial field if you want them. Perhaps arguably this strategy could do that internally? Wether or not that ends up happening, I just want to remove the problematic behavior now. shouldn't index center-points with shapes together will index the center-point of a non-point shape it is given to index, in addition to the shape itself of course. The rationale was that this point could be picked up
******
spatial.src.java.org.apache.lucene.spatial.prefix.PrefixTreeStrategy.PrefixTreeStrategy.createIndexableFields(Shape, double), true
#####
lucene-4.0
LUCENE-4662
https://issues.apache.org/jira/browse/LUCENE-4662
Elision in FrenchAnalyzer It seems org.apache.lucene.analysis.fr.FrenchAnalyzer.DEFAULT_ARTICLES is missing "d" and "c", but also "jusqu", "quoiqu", "lorsqu", and "puisqu".
Elision is missing "d" and "c", but also "jusqu", "quoiqu", "lorsqu", and "puisqu". It seems
******
#####
lucene-4.0
LUCENE-4513
https://issues.apache.org/jira/browse/LUCENE-4513
Deleted nested docs are scored into parent doc. If a nested doc is deleted is still scored into the parent doc, which I think isn't right.
Deleted nested docs are scored into parent doc. If a nested doc is deleted is still scored into the parent doc, which I think isn't right.
******
join.src.java.org.apache.lucene.search.join.ToParentBlockJoinQuery.ToParentBlockJoinQuery.scorer(AtomicReaderContext, boolean, boolean, Bits), true
join.src.test.org.apache.lucene.search.join.TestBlockJoin.TestBlockJoin.testNestedDocScoringWithDeletes(), false, new_method
#####
lucene-4.0
LUCENE-4443
https://issues.apache.org/jira/browse/LUCENE-4443
BlockPostingsFormat writes unnecessary skipdata Seems to me lastStartOffset is unnecessary, when we skip to a document, it implicitly is 0: see BlockPostingsWriter.startDoc. (Unless I'm missing something, all tests pass with "Block" if i remove it) Separately we should really think about lastPayloadByteUpto, is this worth it? instead when we actually skip, we could sum the payloadLengthBuffer from 0..curPosBufferUpto as we are going to decode that block anyway?
as we are going to decode that block anyway? if i remove it) Separately we should really think about is this worth it? instead when we actually skip, we could sum the (Unless I'm missing something, all tests pass with is unnecessary, when we skip to a document, it implicitly is 0: see Seems to me writes unnecessary
******
codecs.src.java.org.apache.lucene.codecs.block.BlockSkipWriter.BlockSkipWriter.BlockSkipWriter(int, int, int, IndexOutput, IndexOutput, IndexOutput), true
codecs.src.java.org.apache.lucene.codecs.block.BlockSkipWriter.BlockSkipWriter.resetSkip(), true
codecs.src.java.org.apache.lucene.codecs.block.BlockSkipWriter.BlockSkipWriter.bufferSkip(int, int, long, long, int, int, int), true
codecs.src.java.org.apache.lucene.codecs.block.BlockSkipWriter.BlockSkipWriter.writeSkipData(int, IndexOutput), true
codecs.src.java.org.apache.lucene.codecs.block.BlockPostingsReader.BlockPostingsReader.BlockPostingsReader(Directory, FieldInfos, SegmentInfo, IOContext, String), true
codecs.src.java.org.apache.lucene.codecs.block.BlockPostingsReader.BlockPostingsReader.init(IndexInput), true
codecs.src.java.org.apache.lucene.codecs.block.BlockPostingsReader.BlockPostingsReader.advance(int), true
codecs.src.java.org.apache.lucene.codecs.block.BlockPostingsWriter.BlockPostingsWriter.startDoc(int, int), true
codecs.src.java.org.apache.lucene.codecs.block.BlockPostingsWriter.BlockPostingsWriter.finishDoc(), true
codecs.src.java.org.apache.lucene.codecs.block.BlockSkipReader.BlockSkipReader.BlockSkipReader(IndexInput, int, int, boolean, boolean, boolean), true
codecs.src.java.org.apache.lucene.codecs.block.BlockSkipReader.BlockSkipReader.getStartOffset(), true
codecs.src.java.org.apache.lucene.codecs.block.BlockSkipReader.BlockSkipReader.seekChild(int), true
codecs.src.java.org.apache.lucene.codecs.block.BlockSkipReader.BlockSkipReader.setLastSkipData(int), true
codecs.src.java.org.apache.lucene.codecs.block.BlockSkipReader.BlockSkipReader.readSkipData(int, IndexInput), true
#####
lucene-4.0
LUCENE-4899
https://issues.apache.org/jira/browse/LUCENE-4899
FastVectorHighlihgter fails with SIOOB if single phrase or term is > fragCharSize This has been reported on several occasions like SOLR-4660 / SOLR-4137 or on the ES mailing list https://groups.google.com/d/msg/elasticsearch/IdyMSPK5gao/nKZq8_NYWmgJ The reason is that the current code expects the fragCharSize > matchLength which is not necessarily true if you use phrases or if you have very long terms like URLs or so. I have a test that reproduces the issue and a fix as far as I can tell (me doesn't have much experience with the highlighter.
if single phrase or term is fails with This has been reported on several occasions like SOLR-4660 / SOLR-4137 or on the ES mailing list https://groups.google.com/d/msg/elasticsearch/IdyMSPK5gao/nKZq8_NYWmgJ The reason is that the current code expects the which is not necessarily true if you use phrases or if you have very long terms like URLs or so. I have a test that reproduces the issue and a fix as far as I can tell (me doesn't have much experience with the highlighter.
******
highlighter.src.test.org.apache.lucene.search.vectorhighlight.FastVectorHighlighterTest.FastVectorHighlighterTest.testPhraseHighlightTest(), false, new_method
highlighter.src.test.org.apache.lucene.search.vectorhighlight.FastVectorHighlighterTest.FastVectorHighlighterTest.testPhraseHighlightLongTextTest(), false, new_method
highlighter.src.java.org.apache.lucene.search.vectorhighlight.BaseFragListBuilder.BaseFragListBuilder.createFieldFragList(FieldPhraseList, FieldFragList, int), true
highlighter.src.java.org.apache.lucene.search.vectorhighlight.BaseFragListBuilder.BaseFragListBuilder.removeTop(), false, refactoring
highlighter.src.java.org.apache.lucene.search.vectorhighlight.BaseFragListBuilder.BaseFragListBuilder.top(), false, refactoring
highlighter.src.java.org.apache.lucene.search.vectorhighlight.BaseFragListBuilder.BaseFragListBuilder.IteratorQueue(Iterator<T>), false, refactoring
highlighter.src.java.org.apache.lucene.search.vectorhighlight.BaseFragListBuilder.BaseFragListBuilder.acceptPhrase(WeightedPhraseInfo, int, int), false, refactoring
highlighter.src.test.org.apache.lucene.search.vectorhighlight.SimpleFragListBuilderTest.SimpleFragListBuilderTest.testSmallerFragSizeThanTermQuery(), false, test_method
highlighter.src.test.org.apache.lucene.search.vectorhighlight.SimpleFragListBuilderTest.SimpleFragListBuilderTest.testSmallerFragSizeThanPhraseQuery(), false, test_method
#####
lucene-4.0
LUCENE-4411
https://issues.apache.org/jira/browse/LUCENE-4411
Depth requested in a facetRequest is reset when Sampling is in effect FacetRequest can be set a Depth parameter, which controls the depth of the result tree to be returned. When Sampling is enabled (and actually used) the Depth parameter gets reset to its default (1).
can be set is reset when Sampling is in effect parameter gets reset to its default (1). parameter, which controls the depth of the result tree to be returned. When Sampling is enabled (and actually used) the requested
******
contrib.facet.src.test.org.apache.lucene.facet.search.sampling.OversampleWithDepthTest.OversampleWithDepthTest.testCountWithdepthUsingSamping(), false, new_method
contrib.facet.src.test.org.apache.lucene.facet.search.sampling.OversampleWithDepthTest.OversampleWithDepthTest.index100Docs(Directory, Directory), false, new_method
contrib.facet.src.test.org.apache.lucene.facet.search.sampling.OversampleWithDepthTest.OversampleWithDepthTest.FacetResult(IndexReader, TaxonomyReader, FacetSearchParams, SamplingParams), false, new_method
contrib.facet.src.java.org.apache.lucene.facet.search.sampling.Sampler.Sampler.OverSampledFacetRequest(FacetRequest, int), true
contrib.facet.src.java.org.apache.lucene.facet.search.sampling.Sampler.Sampler.createCategoryListIterator(IndexReader, TaxonomyReader, FacetSearchParams, int), false, new_method
contrib.facet.src.java.org.apache.lucene.facet.search.sampling.Sampler.Sampler.supportsComplements(), false, new_method
#####
lucene-4.0
LUCENE-4629
https://issues.apache.org/jira/browse/LUCENE-4629
IndexWriter fails to delete documents if Iterator<IndexDocument> throws an exception In DWPT we iterator over a document block and roll back documents if one of the docs fails with a non-aborting exception. Yet, we miss to delete those document if the iterator itself throws an exception. Given the fact that we allow an Iterable on IW we should be prepared for RT exceptions since these documents might be created in a stream fashing rather than already build up. IMO its a valid usecase if you have large documents to not materialize them in memory before indexing or at least we don't require this.
fails to delete documents if throws an exception we iterator over a document block and roll back documents if one of the docs fails with a non-aborting exception. Yet, we miss to delete those document if the iterator itself throws an exception. Given the fact that we allow an Iterable on IW we should be prepared for RT exceptions since these documents might be created in a stream fashing rather than already build up. IMO its a valid usecase if you have large documents to not materialize them in memory before indexing or at least we don't require this.
******
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.testIterableThrowsException(), false, new_method
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.RandomFailingFieldIterable(List<? extends IndexDocument>, Random), false, new_method
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.iterator(), false, new_method
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.hasNext(), false, new_method
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.next(), false, new_method
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.remove(), false, new_method
core.src.java.org.apache.lucene.index.DocumentsWriterPerThread.DocumentsWriterPerThread.updateDocuments(Iterable<? extends IndexDocument>, Analyzer, Term), true
#####
lucene-4.0
LUCENE-4567
https://issues.apache.org/jira/browse/LUCENE-4567
WFSTCompletionLookup.lookup() NPE when empty fst the fst Builder.finish() returns null when nothing is accepted, this then results in NPE in lookup(), see patch for extra nullchecks
when empty  the returns null when nothing is accepted, this then results in see patch for extra
******
suggest.src.test.org.apache.lucene.search.suggest.fst.WFSTCompletionTest.WFSTCompletionTest.testEmpty(), false, new_method
suggest.src.java.org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester.AnalyzingSuggester.store(OutputStream), true
suggest.src.java.org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester.AnalyzingSuggester.lookup(CharSequence, boolean, int), true
suggest.src.test.org.apache.lucene.search.suggest.analyzing.AnalyzingSuggesterTest.AnalyzingSuggesterTest.testEmpty(), false, new_method
suggest.src.java.org.apache.lucene.search.suggest.fst.WFSTCompletionLookup.WFSTCompletionLookup.lookup(CharSequence, boolean, int), true
suggest.src.java.org.apache.lucene.search.suggest.fst.WFSTCompletionLookup.WFSTCompletionLookup.get(CharSequence), true
suggest.src.test.org.apache.lucene.search.suggest.analyzing.FuzzySuggesterTest.FuzzySuggesterTest.testEmpty(), false, new_method
#####
lucene-4.0
LUCENE-4660
https://issues.apache.org/jira/browse/LUCENE-4660
When ConcurrentMergeScheduler stalls incoming threads it has unexpected hysteresis Eg if you set maxMergeCount to 2, as soon as a 3rd merge need to kick off, we stall incoming segment-creating threads. Then we wait ... and we are supposed to resume the threads when the merge count drops back to 2, but instead we are only resuming when merge count gets to 1. Ie, we stall for too long (= unexpected hysteresis).
When stalls incoming threads it has unexpected hysteresis Eg if you set to 2, as soon as a 3rd merge need to kick off, we stall incoming segment-creating threads. Then we wait ... and we are supposed to resume the threads when the merge count drops back to 2, but instead we are only resuming when merge count gets to 1. Ie, we stall for too long (= unexpected hysteresis).
******
core.src.java.org.apache.lucene.index.ConcurrentMergeScheduler.ConcurrentMergeScheduler.run(), true
#####
lucene-4.0
LUCENE-4596
https://issues.apache.org/jira/browse/LUCENE-4596
DirectoryTaxonomyWriter concurrency bug Mike tripped this error while running some benchmarks: {no format} Caused by: java.lang.ArrayIndexOutOfBoundsException: 130 at org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.getParent(DirectoryTaxonomyWriter.java:835) at org.apache.lucene.facet.index.streaming.CategoryParentsStream.incrementToken(CategoryParentsStream.java:106) at org.apache.lucene.facet.index.streaming.CountingListTokenizer.incrementToken(CountingListTokenizer.java:63) at org.apache.lucene.facet.index.streaming.CategoryTokenizer.incrementToken(CategoryTokenizer.java:48) at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:177) at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:272) at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:250) at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:376) at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1455) at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1130) at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1111) at perf.IndexThreads$IndexThread.run(IndexThreads.java:335) At first we thought this might be related to LUCENE-4565, but he reverted to before that commit and still hit the exception. I modified TestDirTaxoWriter.testConcurrency to index hierarchical categories, thinking that's the cause, but failed to reproduce. Eventually I realized that the test doesn't call getParent(), because it tests DirTaxoWriter concurrency, not concurrent indexing. As soon as I added a call to getParent, I hit this exception too. Adding 'synchronized' to DirTaxoWriter.addCategory seems to avoid that ex. I'll upload a patch with the modifications to the test and dig.
concurrency bug Mike tripped this error while running some benchmarks: At first we thought this might be related to but he reverted to before that commit and still hit the exception. I modified to index hierarchical categories, thinking that's the cause, but failed to reproduce. Eventually I realized that the test doesn't call because it tests concurrency, not concurrent indexing. As soon as I added a call to I hit this exception too. Adding seems to avoid that ex.
******
facet.src.java.org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.DirectoryTaxonomyWriter.findCategory(CategoryPath), true
facet.src.java.org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.DirectoryTaxonomyWriter.addCategoryDocument(CategoryPath, int, int), true
facet.src.java.org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.DirectoryTaxonomyWriter.getParent(int), true
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestDirectoryTaxonomyWriter.TestDirectoryTaxonomyWriter.testConcurrency(), false, test_method
facet.src.java.org.apache.lucene.facet.taxonomy.directory.ParallelTaxonomyArrays.ParallelTaxonomyArrays.add(int, int), true
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.close(), false, new_method
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.get(CategoryPath), false, new_method
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.get(CategoryPath, int), false, new_method
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.put(CategoryPath, int), false, new_method
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.put(CategoryPath, int, int), false, new_method
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.isFull(), false, new_method
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.clear(), false, new_method
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.newCategory(), false, new_method
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.newTaxoWriterCache(int), false, new_method
facet.src.test.org.apache.lucene.facet.taxonomy.directory.TestConcurrentFacetedIndexing.TestConcurrentFacetedIndexing.testConcurrency(), false, new_method
#####
lucene-4.0
LUCENE-4618
https://issues.apache.org/jira/browse/LUCENE-4618
reproducable failure of TestMaxFailuresRule james mentioned on the dev list that TestMaxFailuresRule fails reliable for him on multiple platforms when using -Dtests.seed=3FACDC7EBD23CB80:3D65D783617F94F1 I was able to reproduce on my linux box as of trunk r1420486.
james mentioned on the dev list that fails reliable for him on multiple platforms when using I was able to reproduce on my linux box as of trunk r1420486.
******
core.src.test.org.apache.lucene.util.TestMaxFailuresRule.TestMaxFailuresRule.testFailSometimes(), false, test_method
#####
lucene-4.0
LUCENE-4521
https://issues.apache.org/jira/browse/LUCENE-4521
tryDeleteDocument returns true (success) but may fail to write changes to the index on commit Spinoff from java-user thread subject "writer.tryDeleteDocument(..) does not delete document".
returns true (success) but may fail to write changes to the index on commit Spinoff from java-user thread subject does not delete document
******
core.src.java.org.apache.lucene.index.IndexWriter.IndexWriter.tryDeleteDocument(IndexReader, int), true
core.src.test.org.apache.lucene.index.TestIndexWriterDelete.TestIndexWriterDelete.testTryDeleteDocument(), false, new_method
#####
lucene-4.0
LUCENE-4566
https://issues.apache.org/jira/browse/LUCENE-4566
SearcherManager.afterRefresh() issues 1) ReferenceManager.doMaybeRefresh seems to call afterRefresh even if it didn't refresh/swap, (when newReference == null) 2) It would be nice if users were allowed to override SearcherManager.afterRefresh() to get notified when a new searcher is in action. But SearcherManager and ReaderManager are final, while NRTManager is not. The only way to currently hook into when a new searched is created is using the factory, but if you wish to do some async task then, there are no guarantees that acquire() will return the new searcher, so you have to pass it around and incRef manually. While if allowed to hook into afterRefresh you can just rely on acquire() & existing infra you have around it to give you the latest one.
issues seems to call even if it didn't refresh/swap It would be nice if users were allowed to override to get notified when a new searcher is in action. are final, while is not. The only way to currently hook into when a new searched is created is using the factory, but if you wish to do some async task then, there are no guarantees that will return the new searcher, so you have to pass it around and manually. While if allowed to hook into you can just rely on existing infra you have around it to give you the latest one.
******
core.src.java.org.apache.lucene.search.NRTManager.NRTManager.afterRefresh(), false, method_signature
core.src.test.org.apache.lucene.search.TestSearcherManager.TestSearcherManager.testListenerCalled(), false, new_method
core.src.test.org.apache.lucene.search.TestNRTManager.TestNRTManager.testListenerCalled(), false, new_method
core.src.java.org.apache.lucene.search.ReferenceManager.ReferenceManager.doMaybeRefresh(), true
core.src.java.org.apache.lucene.search.ReferenceManager.ReferenceManager.maybeRefreshBlocking(), true
core.src.java.org.apache.lucene.search.ReferenceManager.ReferenceManager.notifyRefreshListeners(), false, new_method
core.src.java.org.apache.lucene.search.ReferenceManager.ReferenceManager.addListener(RefreshListener), false, new_method
core.src.java.org.apache.lucene.search.ReferenceManager.ReferenceManager.removeListener(RefreshListener), false, new_method
#####
lucene-4.0
LUCENE-4656
https://issues.apache.org/jira/browse/LUCENE-4656
Fix IndexWriter working together with EmptyTokenizer and EmptyTokenStream (without CharTermAttribute), fix BaseTokenStreamTestCase TestRandomChains can fail because EmptyTokenizer doesn't have a CharTermAttribute and doesn't compute the end offset (if the offset attribute was added by a filter).
Fix working together with and without can fail because doesn't have a and doesn't compute the end offset (if the offset attribute was added by a filter).
******
core.src.test.org.apache.lucene.document.TestDocument.TestDocument.testInvalidFields(), false, test_method
analysis.common.src.test.org.apache.lucene.analysis.miscellaneous.TestEmptyTokenStream.TestEmptyTokenStream.test(), false, test_method
analysis.common.src.test.org.apache.lucene.analysis.miscellaneous.TestEmptyTokenStream.TestEmptyTokenStream.testConsume2(), false, new_method
analysis.common.src.test.org.apache.lucene.analysis.miscellaneous.TestEmptyTokenStream.TestEmptyTokenStream.testIndexWriter_LUCENE4656(), false, new_method
test-framework.src.java.org.apache.lucene.analysis.BaseTokenStreamTestCase.BaseTokenStreamTestCase.assertTokenStreamContents(TokenStream, String[], int[], int[], String[], int[], int[], Integer, boolean), false, test_method
test-framework.src.java.org.apache.lucene.analysis.BaseTokenStreamTestCase.BaseTokenStreamTestCase.checkAnalysisConsistency(Random, Analyzer, boolean, String, boolean, Field), true
core.src.java.org.apache.lucene.index.DocInverterPerField.DocInverterPerField.processFields(IndexableField[], int), true
queryparser.src.test.org.apache.lucene.queryparser.classic.TestMultiFieldQueryParser.TestMultiFieldQueryParser.createComponents(String, Reader), false, test_method
test-framework.src.java.org.apache.lucene.analysis.EmptyTokenizer.EmptyTokenizer.EmptyTokenizer(Reader), false, test_method
test-framework.src.java.org.apache.lucene.analysis.EmptyTokenizer.EmptyTokenizer.incrementToken(), true
queryparser.src.test.org.apache.lucene.queryparser.flexible.standard.TestMultiFieldQPHelper.TestMultiFieldQPHelper.createComponents(String, Reader), false, test_method
#####
lucene-4.0
LUCENE-4486
https://issues.apache.org/jira/browse/LUCENE-4486
Highlighter doesn't support ConstantScoreQuery if you wrap a query into a constant score the highlighter fails to highlight since ConstantScoreQuery is not recognized in WeightedSpanTermExtractor
doesn't support if you wrap a query into a constant score the fails to highlight since is not recognized in
******
highlighter.src.test.org.apache.lucene.search.highlight.HighlighterTest.HighlighterTest.testGetBestFragmentsConstantScore(), false, new_method
highlighter.src.java.org.apache.lucene.search.highlight.WeightedSpanTermExtractor.WeightedSpanTermExtractor.extract(Query, Map<String, WeightedSpanTerm>), true
#####
lucene-4.0
LUCENE-4657
https://issues.apache.org/jira/browse/LUCENE-4657
a new testThreadInterruptDeadlock failure
a new failure
******
core.src.java.org.apache.lucene.index.IndexWriter.IndexWriter.dropAll(boolean), true
core.src.java.org.apache.lucene.index.IndexWriter.IndexWriter.commitMergedDeletes(MergePolicy.OneMerge), true
core.src.test.org.apache.lucene.index.TestIndexWriter.TestIndexWriter.run(), false, test_method
core.src.java.org.apache.lucene.index.BufferedDeletesStream.BufferedDeletesStream.applyDeletes(IndexWriter.ReaderPool, List<SegmentInfoPerCommit>), true
#####
lucene-4.0
LUCENE-4587
https://issues.apache.org/jira/browse/LUCENE-4587
WordBreakSpellChecker treats bytes as chars Originally opened as SOLR-4115.
treats bytes as chars Originally opened as SOLR-4115.
******
suggest.src.java.org.apache.lucene.search.spell.WordBreakSpellChecker.WordBreakSpellChecker.suggestWordCombinations(Term[], int, IndexReader, SuggestMode), true
suggest.src.java.org.apache.lucene.search.spell.WordBreakSpellChecker.WordBreakSpellChecker.generateBreakUpSuggestions(Term, IndexReader, int, int, int, SuggestWord[], Queue<SuggestWordArrayWrapper>, int, BreakSuggestionSortMethod), true
suggest.src.java.org.apache.lucene.search.spell.WordBreakSpellChecker.WordBreakSpellChecker.generateSuggestWord(IndexReader, BytesRef, int, int, String), true
suggest.src.test.org.apache.lucene.search.spell.TestWordBreakSpellChecker.TestWordBreakSpellChecker.goodTestString(String), false, new_method
suggest.src.test.org.apache.lucene.search.spell.TestWordBreakSpellChecker.TestWordBreakSpellChecker.testRandom(), false, new_method
suggest.src.test.org.apache.lucene.search.spell.TestWordBreakSpellChecker.TestWordBreakSpellChecker.testBreakingWords(), false, new_method
#####
lucene-4.0
LUCENE-4466
https://issues.apache.org/jira/browse/LUCENE-4466
Bounds check inconsistent for stored fields vs term vectors  SegmentReader.document does the check for stored fields. Codec's dont. SegmentReader.getTermVectors doesnt do the check for vectors. Codec does. I think we should move the vectors check out to SR, too. Codecs can have an assert if they want, but the APIs should look more consistent.
Bounds check inconsistent for stored fields vs term vectors  does the check for stored fields. Codec's dont. doesnt do the check for vectors. Codec does. I think we should move the vectors check out to SR, too. Codecs can have an assert if they want, but the APIs should look more consistent.
******
core.src.java.org.apache.lucene.codecs.lucene40.Lucene40TermVectorsReader.Lucene40TermVectorsReader.get(int), true
core.src.test.org.apache.lucene.index.TestTermVectorsReader.TestTermVectorsReader.testBadParams(), false, test_method
core.src.test.org.apache.lucene.index.TestSegmentReader.TestSegmentReader.testOutOfBoundsAccess(), false, test_method
core.src.java.org.apache.lucene.index.SegmentReader.SegmentReader.document(int, StoredFieldVisitor), true
core.src.java.org.apache.lucene.index.SegmentReader.SegmentReader.getTermVectors(int), true
core.src.java.org.apache.lucene.index.SegmentReader.SegmentReader.checkBounds(int), false, new_method
codecs.src.java.org.apache.lucene.codecs.simpletext.SimpleTextTermVectorsReader.SimpleTextTermVectorsReader.get(int), true
#####
lucene-4.0
LUCENE-4615
https://issues.apache.org/jira/browse/LUCENE-4615
Remove Int/FloatArrayAllocator from facet module? Spinoff from LUCENE-4600. It makes me nervous to have allocation tied to our public APIs ... and the ability for Int/FloatArrayAllocator to hold onto N arrays indefinitely makes me even more nervous. I think we should just trust java/GC to do their job here and free the storage as soon as faceting is done.
Remove from facet module? Spinoff from LUCENE-4600. It makes me nervous to have allocation tied to our public APIs ... and the ability for to hold onto arrays indefinitely makes me even more nervous. I think we should just trust java/GC to do their job here and free the storage as soon as faceting is done.
******
facet.src.test.org.apache.lucene.facet.search.TestTopKInEachNodeResultHandler.TestTopKInEachNodeResultHandler.testSimple(), false, test_method
facet.src.java.org.apache.lucene.facet.search.TemporaryObjectAllocator.TemporaryObjectAllocator.TemporaryObjectAllocator(int), true
facet.src.java.org.apache.lucene.facet.search.TemporaryObjectAllocator.TemporaryObjectAllocator.create(), true
facet.src.java.org.apache.lucene.facet.search.TemporaryObjectAllocator.TemporaryObjectAllocator.clear(T), true
facet.src.java.org.apache.lucene.facet.search.TemporaryObjectAllocator.TemporaryObjectAllocator.allocate(), true
facet.src.java.org.apache.lucene.facet.search.TemporaryObjectAllocator.TemporaryObjectAllocator.free(T), true
facet.src.java.org.apache.lucene.facet.search.ArraysPool.ArraysPool.ArraysPool(int, int), false, new_method
facet.src.java.org.apache.lucene.facet.search.ArraysPool.ArraysPool.allocateIntArray(), false, new_method
facet.src.java.org.apache.lucene.facet.search.ArraysPool.ArraysPool.allocateFloatArray(), false, new_method
facet.src.java.org.apache.lucene.facet.search.ArraysPool.ArraysPool.free(int[]), false, new_method
facet.src.java.org.apache.lucene.facet.search.ArraysPool.ArraysPool.free(float[]), false, new_method
facet.src.test.org.apache.lucene.facet.search.TestFacetArrays.TestFacetArrays.testSimple(), false, test_method
facet.src.java.org.apache.lucene.facet.search.sampling.SamplingAccumulator.SamplingAccumulator.SamplingAccumulator(Sampler, FacetSearchParams, IndexReader, TaxonomyReader, IntArrayAllocator, FloatArrayAllocator), true
facet.src.java.org.apache.lucene.facet.search.sampling.SamplingAccumulator.SamplingAccumulator.AdaptiveFacetsAccumulator(FacetSearchParams, IndexReader, TaxonomyReader, IntArrayAllocator, FloatArrayAllocator), true
facet.src.java.org.apache.lucene.facet.search.FacetArrays.FacetArrays.FacetArrays(IntArrayAllocator, FloatArrayAllocator), true
facet.src.java.org.apache.lucene.facet.search.FacetArrays.FacetArrays.free(), true
facet.src.java.org.apache.lucene.facet.search.FacetArrays.FacetArrays.getIntArray(), true
facet.src.java.org.apache.lucene.facet.search.FacetArrays.FacetArrays.getFloatArray(), true
facet.src.java.org.apache.lucene.facet.search.FacetArrays.FacetArrays.getArraysLength(), true
facet.src.java.org.apache.lucene.facet.search.FacetArrays.FacetArrays.newFloatArray(), true
facet.src.java.org.apache.lucene.facet.search.FacetArrays.FacetArrays.newIntArray(), true
facet.src.java.org.apache.lucene.facet.search.FacetArrays.FacetArrays.doFree(float[], int[]), true
facet.src.java.org.apache.lucene.facet.search.StandardFacetsAccumulator.StandardFacetsAccumulator.accumulate(ScoredDocIDs), true
facet.src.java.org.apache.lucene.facet.search.StandardFacetsAccumulator.StandardFacetsAccumulator.StandardFacetsAccumulator(FacetSearchParams, IndexReader, TaxonomyReader), true
facet.src.java.org.apache.lucene.facet.search.StandardFacetsAccumulator.StandardFacetsAccumulator.StandardFacetsAccumulator(FacetSearchParams, IndexReader, TaxonomyReader, IntArrayAllocator, FloatArrayAllocator), true
facet.src.java.org.apache.lucene.facet.search.TopKInEachNodeHandler.TopKInEachNodeHandler.fetchPartitionResult(FacetArrays, int), true
facet.src.java.org.apache.lucene.facet.search.FacetResultsHandler.FacetResultsHandler.isSelfPartition(int, FacetArrays, int), true
facet.src.java.org.apache.lucene.facet.search.IntArrayAllocator.IntArrayAllocator.clear(int[]), true
facet.src.java.org.apache.lucene.facet.search.IntArrayAllocator.IntArrayAllocator.IntArrayAllocator(int, int), true
facet.src.java.org.apache.lucene.facet.search.IntArrayAllocator.IntArrayAllocator.create(), true
facet.src.java.org.apache.lucene.facet.search.FloatArrayAllocator.FloatArrayAllocator.FloatArrayAllocator(int, int), true
facet.src.java.org.apache.lucene.facet.search.FloatArrayAllocator.FloatArrayAllocator.create(), true
facet.src.java.org.apache.lucene.facet.search.FloatArrayAllocator.FloatArrayAllocator.clear(float[]), true
facet.src.java.org.apache.lucene.facet.search.TopKFacetResultsHandler.TopKFacetResultsHandler.fetchPartitionResult(FacetArrays, int), true
facet.src.java.org.apache.lucene.facet.search.TopKFacetResultsHandler.TopKFacetResultsHandler.heapDescendants(int, Heap<FacetResultNode>, MutableFacetResultNode, FacetArrays, int), true
facet.src.java.org.apache.lucene.facet.search.ReusingFacetArrays.ReusingFacetArrays.ReusingFacetArrays(ArraysPool), true
facet.src.java.org.apache.lucene.facet.search.ReusingFacetArrays.ReusingFacetArrays.newIntArray(), true
facet.src.java.org.apache.lucene.facet.search.ReusingFacetArrays.ReusingFacetArrays.newFloatArray(), true
facet.src.java.org.apache.lucene.facet.search.ReusingFacetArrays.ReusingFacetArrays.doFree(float[], int[]), true
#####
lucene-4.0
LUCENE-4639
https://issues.apache.org/jira/browse/LUCENE-4639
Improving _TestUtil.getTempDir Spinoff from here: http://lucene.472066.n3.nabble.com/TestUtil-getTempFile-may-fail-on-quot-Access-Denied-quot-td4028048.html. _TestUtil.getTempDir uses createTempFile and then deletes the file. While this usually works, if someone runs tests by multiple JVMs and does not ensure each JVM gets an isolated temp.dir to work in, that my result in two JVMs sharing the same directory. Also, on Windows, if you call getTempDir on an existing directory, you get an "Access is denied" exception. Dawid proposed a simple solution to just call mkdirs() continuously until success. I'd like to try that. Also, I think that genTempFile could use some house cleaning, e.g.: tempFileLocker can be just an Object instance? Why do we need a class? If we initialize counter and counterBase in a static clause, we can avoid checking if counter==0 as well as passing Random to genTempFile (that will remove any suspicion that it does anything randomly) Also, instead of synchronizing on tempFileLocker, can we just use AtomicInteger for the counter? I'll modify getTempDir first. It documents "does not create the directory", I want to make sure no test fails due that.
Spinoff from here: http://lucene.472066.n3.nabble.com/TestUtil-getTempFile-may-fail-on-quot-Access-Denied-quot-td4028048.html. uses and then deletes the file. While this usually works, if someone runs tests by multiple JVMs and does not ensure each JVM gets an isolated to work in, that my result in two JVMs sharing the same directory. Also, on Windows, if you call on an existing directory, you get an "Access is denied" exception. Dawid proposed a simple solution to just call continuously until success. I'd like to try that. Also, I think that could use some house cleaning, e.g. can be just an Object instance? Why do we need a class? If we initialize counter and in a static clause, we can avoid checking if as well as passing (that will remove any suspicion that it does anything randomly) Also, instead of synchronizing on can we just use for the counter? I'll modify first. It documents "does not create the directory", I want to make sure no test fails due that.
******
core.src.test.org.apache.solr.core.TestSolrXMLSerializer.TestSolrXMLSerializer.basicUsageTest(), false, test_method
core.src.test.org.apache.lucene.store.TestFileSwitchDirectory.TestFileSwitchDirectory.newFSSwitchDirectory(Set<String>), false, test_method
core.src.test.org.apache.lucene.store.TestFileSwitchDirectory.TestFileSwitchDirectory.testNoDir(), false, test_method
core.src.test.org.apache.lucene.store.TestNRTCachingDirectory.TestNRTCachingDirectory.testNoDir(), false, test_method
test-framework.src.java.org.apache.lucene.util._TestUtil._TestUtil.getTempDir(String), false, test_method
test-framework.src.java.org.apache.lucene.util._TestUtil._TestUtil.createTempFile(String, String, File), true
test-framework.src.java.org.apache.lucene.util._TestUtil._TestUtil.genTempFile(Random, String, String, File), true
core.src.test.org.apache.solr.servlet.CacheHeaderTest.CacheHeaderTest.makeFile(String, String), false, test_method
core.src.test.org.apache.lucene.index.TestDirectoryReader.TestDirectoryReader.testNoDir(), false, test_method
core.src.test.org.apache.lucene.index.TestIndexWriterLockRelease.TestIndexWriterLockRelease.setUp(), false, test_method
core.src.test.org.apache.lucene.index.TestIndexWriterLockRelease.TestIndexWriterLockRelease.tearDown(), true
core.src.test.org.apache.lucene.index.TestIndexWriterLockRelease.TestIndexWriterLockRelease.testIndexWriterLockRelease(), false, test_method
#####
